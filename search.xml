<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kafka 环境部署]]></title>
    <url>%2FKafka%2FKafka-%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前期准备Kafka概述访问 Kafka 官网了解 Kafka 是什么、能做什么、如何使用。 Kafka 安装前置条件安装 ZooKeeper 下载 Kafka 与解压下载地址 找到 kafka_2.11-0.9.0.0.tgz 并下载 将他传输到 /abs/software 目录 解压到指定目录 123#将Kafka解压到/abs/app/中tar -zxvf kafka_2.11-0.9.0.0.tgz -C /abs/app/ 配置 kafka 环境变量将 kafka 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport KAFKA_HOME=/abs/app/kafka_2.11-0.9.0.0export PATH=$KAFKA_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile Kafka单节点单broker的部署及使用修改配置文件在 /abs/app/tmp 目录下新建 kafka-logs 文件夹: 1mkdir kafka-logs 在 /abs/app/kafka_2.11-0.9.0.0/config 目录中对 server.properties 进行修改 123456789101112131415161718192021# 编辑配置文件vi server.properties# The id of the broker. This must be set to a unique integer for each broker.broker.id=0# The port the socket server listens on#port=9092listeners=PLAINTEXT://:9092# Hostname the broker will bind to. If not set, the server will bind to all interfaceshost.name=hadoop# A comma seperated list of directories under which to store log fileslog.dirs=/abs/app/tmp/kafka-logs# The default number of log partitions per topic. More partitions allow greaternum.partitions=1# root directory for all kafka znodes.zookeeper.connect=hadoop:2181 启动 Kafka先启动 ZooKeeper 参照 Kafka 官网快速启动 在前台启动单节点单 broker 的 Kafka 在 /abs/app/kafka_2.11-0.9.0.0/bin 目录中进行如下操作: 1kafka-server-start.sh $KAFKA_HOME/config/server.properties 验证启动 Kafka用 jps 查看当前所有的 Java 进程 pid如下: 123456789[root@hadoop tmp]# jps2448 QuorumPeerMain2595 Jps2538 Kafka[root@hadoop tmp]# jps -m2448 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2538 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server.properties2606 Jps -m 可以看到 Kafka 进程已经启动 创建一个 topic1kafka-topics.sh --create --zookeeper hadoop:2181 --replication-factor 1 --partitions 1 --topic hello_topic (PS: 创建 topic 相当于指定 zookeeper) 查看所有 topic1kafka-topics.sh --list --zookeeper hadoop:2181 发送消息1kafka-console-producer.sh --broker-list hadoop:9092 --topic hello_topic (PS: 发送消息相当于指定 broker) 消费消息1kafka-console-consumer.sh --zookeeper hadoop:2181 --topic hello_topic --from-beginning (PS: 消费消息相当于指定 zookeeper , from-beginning 的意思为是否从开始消费，根据实际需求决定是否添加) 验证发送消息与消费消息在发送端发送消息，看看在消费消息端是否能看到。 查看 topic 详情查看所有 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 查看指定 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic hello_topic Kafka单节点多broker的部署及使用修改配置文件参照 Kafka 官网快速启动进行相关配置 在 /abs/app/kafka_2.11-0.9.0.0/config 目录中为 broker 新建三个配置文件 1234# make a config file for each of the brokerscp server.properties server-1.propertiescp server.properties server-2.propertiescp server.properties server-3.properties 对 server-1.properties 、 server-2.properties 、 server-3.properties 进行相关配置 1234567891011121314151617181920# config server-1.propertiesvi server-1.propertiesbroker.id=1listeners=PLAINTEXT://:9093log.dirs=/abs/app/tmp/kafka-logs-1# config server-2.propertiesvi server-2.propertiesbroker.id=2listeners=PLAINTEXT://:9094log.dirs=/abs/app/tmp/kafka-logs-2# config server-3.propertiesvi server-3.propertiesbroker.id=3listeners=PLAINTEXT://:9095log.dir=/tmp/kafka-logs-3 启动 Kafka先启动 ZooKeeper 参照 Kafka 官网快速启动 在后台启动单节点多 broker 的 Kafka 在 /abs/app/kafka_2.11-0.9.0.0/bin 目录中进行如下操作: 12345kafka-server-start.sh -daemon $KAFKA_HOME/config/server-1.properties &amp;kafka-server-start.sh -daemon $KAFKA_HOME/config/server-2.properties &amp;kafka-server-start.sh -daemon $KAFKA_HOME/config/server-3.properties &amp; 验证启动 Kafka用 jps 查看当前所有的 Java 进程 pid如下: 1234567891011121314[root@hadoop config]# jps2577 Kafka2451 QuorumPeerMain2520 Kafka2633 Kafka2686 Jps[3]+ Done kafka-server-start.sh -daemon $KAFKA_HOME/config/server-3.properties[root@hadoop config]# jps -m2577 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-2.properties2451 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2520 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-1.properties2633 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-3.properties2697 Jps -m 可以看到 Kafka 进程已经启动。 创建一个 topic1kafka-topics.sh --create --zookeeper hadoop:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic (PS: 创建 topic 相当于指定 zookeeper，这里是一个分区三个副本) 查看所有 topic1kafka-topics.sh --list --zookeeper hadoop:2181 发送消息1kafka-console-producer.sh --broker-list hadoop:9093,hadoop:9094,hadoop:9095 --topic my-replicated-topic (PS: 发送消息相当于指定 broker) 消费消息1kafka-console-consumer.sh --zookeeper hadoop:2181 --topic my-replicated-topic (PS: 消费消息相当于指定 zookeeper , from-beginning 的意思为是否从开始消费，根据实际需求决定是否添加) 验证发送消息与消费消息在发送端发送消息，看看在消费消息端是否能看到。 查看 topic 详情查看所有 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 查看指定 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic my-replicated-topic Kafka 容错性测试前期准备容错测试基于 Kafka单节点多broker的部署及使用 查看指定 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic my-replicated-topic 结果如下: 12Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 可以得出有一个副本、三个分区，主节点是1号分区，在运行着的有1、2、3 用 jps -m 查看当前所有的 Java 进程 pid 12345672640 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-3.properties2738 ConsoleProducer --broker-list hadoop:9093,hadoop:9094,hadoop:9095 --topic my-replicated-topic2581 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-2.properties2759 ConsoleConsumer --zookeeper hadoop:2181 --topic my-replicated-topic2522 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-1.properties2492 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2958 Jps -m 验证发送消息与消费消息1关闭从节点的进程 server-2.properties ，进程号可以看出是 2581，命令如下: 1kill -9 2581 再查看当前所有的 Java 进程 1234567[root@hadoop tmp]# jps -m2640 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-3.properties2738 ConsoleProducer --broker-list hadoop:9093,hadoop:9094,hadoop:9095 --topic my-replicated-topic2759 ConsoleConsumer --zookeeper hadoop:2181 --topic my-replicated-topic2522 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-1.properties2492 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2990 Jps -m 在发送端发送消息，看看在消费消息端是否能看到。 经过测试发现可以从消费信息端接收到发送消息端发出的消息。 然而查看 my-replicated-topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic my-replicated-topic 结果如下: 12Topic:my-replicated-topic PartitionCount:1 Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 1 Replicas: 1,2,3 Isr: 1,3 可以得出有一个副本、三个分区，主节点是1号分区，在运行着的有1、3 2号从节点的故障并没有影响 Kafka 的正常运行 验证发送消息与消费消息2关闭主节点的进程 server-1.properties ，进程号可以看出是 2522，命令如下: 1kill -9 2522 在发送端发送消息，看看在消费消息端是否能看到。 经过测试发现可以从消费信息端接收到发送消息端发出的消息。 然而查看 my-replicated-topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic my-replicated-topic 结果如下: 12Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 3 Replicas: 1,2,3 Isr: 3 可以得出有一个副本、三个分区，主节点是3号，在运行着的只有3号节点 1、2号从节点的故障并没有影响 Kafka 的正常运行，自动选举3号节点为主节点 Kafka 的容错性得到了证明]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 的安装]]></title>
    <url>%2FZooKeeper%2FZooKeeper-%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装步骤下载 ZooKeeper 与解压ZooKeeper 下载地址 找到 zookeeper-3.4.5-cdh5.7.0.tar.gz 将他传输到 /abs/software 目录 解压到指定目录 123#将ZooKeeper解压到/abs/app/中tar -zxvf zookeeper-3.4.5-cdh5.7.0.tar.gz -C /abs/app/ 配置 ZooKeeper 环境变量将 ZooKeeper 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport ZOOKEEPER_HOME=/abs/app/zookeeper-3.4.5-cdh5.7.0export PATH=$ZOOKEEPER_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 配置 ZooKeeper在 /abs/app/zookeeper-3.4.5-cdh5.7.0/conf 目录对 ZooKeeper 进行相关配置 123456cp zoo_sample.cfg zoo.cfgvi zoo.cfg# 修改数据存储文件目录(需要自己建相关目录)dataDir=/abs/app/tmp/zk 验证在 /abs/app/zookeeper-3.4.5-cdh5.7.0/bin 目录输入 ./zkServer.sh start 验证 在命令行显示如下: 123JMX enabled by defaultUsing config: /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 在命令行输入 jps 122563 QuorumPeerMain2581 Jps QuorumPeerMain 就是 ZooKeeper 的进程 连接 ZooKeeper 12345678./zkCli.sh[zk: localhost:2181(CONNECTED) 6] ls /[zookeeper][zk: localhost:2181(CONNECTED) 7] ls /zookeeper[quota][zk: localhost:2181(CONNECTED) 8] ls /zookeeper/quota[] 以上操作没问题就说明安装配置成功。]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume 实战练习]]></title>
    <url>%2FFlume%2FFlume-%E5%AE%9E%E6%88%98%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前期准备了解Flume 架构及核心组件Flume 架构及核心组件 Source : 收集（指定数据源从哪里获取） Channel : 聚集 Sink : 输出（把数据写到哪里去） 学习使用 Flume通过一个简单的小例子学习使用 Flume 使用 Flume 的关键就是写配置文件 配置文件的构成： A) 配置 SourceB) 配置 ChannelC) 配置 SinkD) 把以上三个组件串起来 A simple example 123456789101112131415161718192021222324252627282930313233# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# a1: agent 的名称# r1: source 的名称# k1: sink 的名称# c1: channel 的名称# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# type: source组件的类型# bind: source绑定的主机或IP# port: source绑定的端口号# Describe the sinka1.sinks.k1.type = logger# 把日志输出到控制台# Use a channel which buffers events in memorya1.channels.c1.type = memory# 存放在内存队列# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1# r1的channels指定到c1# k1的channel从c1得到# 一个source可以输出到多个channel# 一个channel只能输出一个sink 实战一需求需求：从指定网络端口采集数据输出到控制台 写配置文件在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录中新建 example.conf 如下: 1234567891011121314151617181920# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = hadoopa1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 agentFlume 官网启动 agent 的命令: 1$ bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template agent options: 123--name,-n &lt;name&gt; the name of this agent (required)--conf,-c &lt;conf&gt; use configs in &lt;conf&gt; directory--conf-file,-f &lt;file&gt; specify a config file (required if -z missing) 实际用的启动 agent 的命令: 1flume-ng agent -n a1 -c $FLUME_HOME $FLUME_HOME/conf/example.conf -Dflume.root.logger=INFO,console // Dflume.root.logger=INFO,console 为将输出结果显示到控制台 启动失败12345Info: Including Hive libraries found via () for Hive access+ exec /abs/app/jdk1.8.0_161/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp '/abs/app/apache-flume-1.6.0-cdh5.7.0-bin:/abs/app/apache-flume-1.6.0-cdh5.7.0-bin/lib/*:/lib/*' -Djava.library.path= org.apache.flume.node.Application -n a1 -f /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf/example.conflog4j:WARN No appenders could be found for logger (org.apache.flume.lifecycle.LifecycleSupervisor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 上网查了一下，别人是 -c 的路径指定错误，我的也错了。 -c 后面跟的是 Flume 的 conf 目录 所以正确的启动命令为: 1flume-ng agent -n a1 -c $FLUME_HOME/conf -f $FLUME_HOME/conf/example.conf -Dflume.root.logger=INFO,console 正常启动后可以看到如下: 可以看到 Sink 和 Source 都启动了 绑定的主机名为 hadoop 的 IP 和绑定的端口号都有显示 验证12[root@hadoop ~]# telnet hadoop 44444-bash: telnet: command not found 显示找不到 telnet ，用 yum install telnet 安装telnet telnet 进入 hadoop 的 44444 端口进行输入单词按 Enter agent 的那一端显示如下: 从图中可以看到如下: 1Event: &#123; headers:&#123;&#125; body: 73 70 61 72 6B 0D spark. &#125; Event 是 Flume 数据传输的基本单元Event = 可选的 header + byte array 以上实现了从指定网络端口采集数据输出到控制台的需求。 实战二需求需求：监控一个文件实时采集新增的数据输出到控制台 根据需求可以采用以下方案实现： Agent 选型: exec source + memory channel + logger sink 写配置文件在 /abs/data 目录新建 data.log 1touch data.log 在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录中新建 exec-memory-logger.conf 如下: 1234567891011121314151617181920# exec-memory-logger.conf: A realtime single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /abs/data/data.loga1.sources.r1.shell = /bin/sh -c# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 agent Flume 启动 agent 的命令: 1flume-ng agent -n a1 -c $FLUME_HOME/conf -f $FLUME_HOME/conf/exec-memory-logger.conf -Dflume.root.logger=INFO,console // Dflume.root.logger=INFO,console 为将输出结果显示到控制台 正常启动后可以看到如下: 可以看到 Source 、 Channel 和 Sink 的类型和启动类型以及 Source 要执行的命令 验证在 /abs/data 目录输入 echo hello &gt;&gt; data.log agent 的那一端显示如下: 以上实现了监控一个文件实时采集新增的数据输出到控制台的需求。 拓展参照 Flume 用户指南 如果用 Flume 采集数据做离线处理，可以使用 HDFS Sink 如果用 Flume 采集数据做实时处理，可以使用 Kafka Sink 这里只提供一个拓展，根据具体的需求使用。 实战三需求需求：将 A 服务器上的日志实时采集到 B 服务器 根据需求可以采用以下方案实现： Agent A 选型: exec source + memory channel + avro sinkAgent B 选型: avro source + memory channel + logger sink 写配置文件在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录中新建如下配置文件: exec-memory-avro.conf: 12345678910111213141516171819202122# exec-memory-avro.conf: A realtime Flume configuration# Name the components on this agentexec-memory-avro.sources = exec-sourceexec-memory-avro.sinks = avro-sinkexec-memory-avro.channels = memory-channel# Describe/configure the sourceexec-memory-avro.sources.exec-source.type = execexec-memory-avro.sources.exec-source.command = tail -F /abs/data/data.logexec-memory-avro.sources.exec-source.shell = /bin/sh -c# Describe the sinkexec-memory-avro.sinks.avro-sink.type = avroexec-memory-avro.sinks.avro-sink.hostname = hadoopexec-memory-avro.sinks.avro-sink.port = 44444# Use a channel which buffers events in memoryexec-memory-avro.channels.memory-channel.type = memory# Bind the source and sink to the channelexec-memory-avro.sources.exec-source.channels = memory-channelexec-memory-avro.sinks.avro-sink.channel = memory-channel avro-memory-logger.conf: 1234567891011121314151617181920# avro-memory-logger.conf: A realtime Flume configuration# Name the components on this agentavro-memory-logger.sources = avro-sourceavro-memory-logger.sinks = logger-sinkavro-memory-logger.channels = memory-channel# Describe/configure the sourceavro-memory-logger.sources.avro-source.type = avroavro-memory-logger.sources.avro-source.bind = hadoopavro-memory-logger.sources.avro-source.port = 44444# Describe the sinkavro-memory-logger.sinks.logger-sink.type = logger# Use a channel which buffers events in memoryavro-memory-logger.channels.memory-channel.type = memory# Bind the source and sink to the channelavro-memory-logger.sources.avro-source.channels = memory-channelavro-memory-logger.sinks.logger-sink.channel = memory-channel 启动 agent两个 Agent ,先启动 Agent A ,再启动 Agent B 先启动 avro-memory-logger: 1flume-ng agent -n avro-memory-logger -c $FLUME_HOME/conf -f $FLUME_HOME/conf/avro-memory-logger.conf -Dflume.root.logger=INFO,console 再启动 exec-memory-avro: 1flume-ng agent -n exec-memory-avro -c $FLUME_HOME/conf -f $FLUME_HOME/conf/exec-memory-avro.conf -Dflume.root.logger=INFO,console 验证在 /abs/data/ 目录中输入以下命令： 12echo hello spark &gt;&gt; data.logecho Valentine &gt;&gt; data.log Agent avro-memory-logger 显示如下： 以上实现了将 A 服务器上的日志实时采集到 B 服务器的需求。 这里采用的是一个服务器开三个窗口，有条件的可以尝试用两台服务器进行这个实战练习]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
        <tag>Practise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume 环境部署]]></title>
    <url>%2FFlume%2FFlume-%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[步骤Flume概述访问 Flume 官网了解 Flume 是什么、能做什么、如何使用。 Flume 安装前置条件Flume 安装前置条件如下： 123456789101112131415Java Runtime Environment - Java 1.8 or later#Java 运行环境 - Java 1.8 或以上 Memory - Sufficient memory for configurations used by sources, channels or sinks#内存 - 足够的内存配置供 sources、channels、sinks 使用Disk Space - Sufficient disk space for configurations used by channels or sinks#磁盘空间 - 足够的磁盘空间配置供 sources、channels、sinks 使用Directory Permissions - Read/Write permissions for directories used by agent# 目录权限 - 赋予 agent 对目录的读写权限 安装 JDK参照 ： Java8 的安装 下载 Flume 与解压下载地址 找到 flume-ng-1.6.0-cdh5.7.0.tar.gz 将他传输到 /abs/software 目录 解压到指定目录 123#将Flume解压到/abs/app/中tar -zxvf flume-ng-1.6.0-cdh5.7.0.tar.gz -C /abs/app/ 配置 Flume 环境变量将 Flume 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport FLUME_HOME=/abs/app/apache-flume-1.6.0-cdh5.7.0-binexport PATH=$FLUME_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 配置 Flume在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录对 Flume 进行相关配置 拷贝 Flume 环境配置脚本 1cp flume-env.sh.template flume-env.sh 1234567#编辑 flume-env.shvi flume-env.sh# 在# export JAVA_HOME=/usr/lib/jvm/java-6-sun下面添加如下配置export JAVA_HOME=/abs/app/jdk1.8.0_161 验证在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/bin 目录输入 flume-ng version 验证 出现以下内容说明安装配置成功。 12345Flume 1.6.0-cdh5.7.0Source code repository: https://git-wip-us.apache.org/repos/asf/flume.gitRevision: 8f5f5143ae30802fe79f9ab96f893e6c54a105d1Compiled by jenkins on Wed Mar 23 11:38:48 PDT 2016From source with checksum 50b533f0ffc32db9246405ac4431872e]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 CentOS7.2 环境编译 Spark-2.2.0 源码]]></title>
    <url>%2FSpark%2F%E5%9F%BA%E4%BA%8E-CentOS7-2-%E7%8E%AF%E5%A2%83%E7%BC%96%E8%AF%91-Spark-2-2-0-%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[编译 Spark-2.2.0 源码参考 基于CentOS6.4环境编译Spark-2.1.0源码 需求分析实际工作中，Spark 官网所提供的安装包不能满足我们的需求。因为环境的不同出现问题所以必须要结合实际环境用 Spark 源码进行编译后使用。 根据 Spark 官方文档编译模块的介绍如下： 1The Maven-based build is the build of reference for Apache Spark. Building Spark using Maven requires Maven 3.3.9 or newer and Java 8+. Note that support for Java 7 was removed as of Spark 2.2.0. 得出编译 Spark-2.2.0 需要 Maven 3.3.9 和 Java 8+ 前期准备 Java8 的安装 Maven3.3.9 的安装 Spark-2.2.0 源码下载Spark 下载地址 解压将下载好的 spark-2.2.0.tgz 通过 Xftp 传输到 /abs/software 目录 解压 spark-2.2.0.tgz 123# 将Spark源码包解压到/abs/app/中tar -zxvf spark-2.2.0.tgz -C /abs/app/ 解压后的目录机构如下所示： 编译查看 Spark 官方文档编译源码部分 我们可以使用 Spark 源码目录中的 dev 下的 make-distribution.sh 脚本，官方提供的编译命令如下： 1./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phadoop-2.7 -Phive -Phive-thriftserver -Pmesos -Pyarn 可以根据具体的条件来编译 Spark，比如我们使用的 Hadoop 版本是 2.6.0-cdh5.7.0，并且我们需要将 Spark 运行在 YARN 上、支持对 Hive 的操作，那么我们的 Spark 源码编译脚本就是： 1./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>编译</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven3.3.9 的安装]]></title>
    <url>%2FJava%2FMaven3-3-9-%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Maven3.3.9 在 Windows10 中的安装与配置说明为保证开发过程中版本的一致性，在虚拟机中 Maven 的版本为 3.3.9 所以物理机中 Maven 的版本为 3.3.9 版本不一致的时候先卸载 Maven ，再安装合适的版本 JDK 的版本也要一致，避免不必要的问题出现 卸载 Maven由于在物理机中的 Maven 版本不一致，所以先卸载再安装。 Maven 由于安装的时候只是解压，配置环境变量，设置本地仓库，所以卸载的时候也很简单 卸载过程如下： 1、删除解压的 Maven 文件夹； 2、右键我的电脑-属性-高级系统设置-环境变量-系统变量-Path，删除path里添加的 Maven 环境变量 D:\ST\IDEA\apache-maven-3.5.0\bin 3、删除本地仓库； 下载与解压 Maven 安装包Maven3.3.9 安装包下载地址 下载 apache-maven-3.3.9-bin.zip 在相应目录中解压 apache-maven-3.3.9-bin.zip 配置 Maven 环境变量右键我的电脑-属性-高级系统设置-环境变量-系统变量-Path 添加一条环境变量的记录 D:\ST\IDEA\apache-maven-3.3.9\bin 验证 Maven 安装的有效性按 Windows + r 输入 cmd 进入命令行界面 在命令行中输入 mvn -v 验证，出现以下内容说明安装配置成功。 123456Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: D:\ST\IDEA\apache-maven-3.3.9\bin\..Java version: 1.8.0_161, vendor: Oracle CorporationJava home: D:\ST\JAVA\jdk\route\jreDefault locale: zh_CN, platform encoding: GBKOS name: "windows 10", version: "10.0", arch: "amd64", family: "dos" Maven3.3.9 在 CentOS7 中的安装与配置下载与解压 Maven 安装包Maven3.3.9 安装包下载地址 下载 apache-maven-3.3.9-bin.tar.gz 用 Xftp 将本地下载好的 JDK 包传输到 /abs/software 目录 解压到指定目录 123# 将Maven解压到/abs/app/中tar -zxvf apache-maven-3.3.9-bin.tar.gz -C /abs/app/ 配置 Maven 环境变量将 Maven 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport MAVEN_HOME=/abs/app/apache-maven-3.3.9export PATH=$MAVEN_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 验证 Maven 安装的有效性在命令行中输入 mvn -v 验证，出现以下内容说明安装配置成功。 123456Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: /abs/app/apache-maven-3.3.9Java version: 1.8.0_161, vendor: Oracle CorporationJava home: /abs/app/jdk1.8.0_161/jreDefault locale: en_US, platform encoding: UTF-8OS name: "linux", version: "3.10.0-327.el7.x86_64", arch: "amd64", family: "unix"]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 的安装]]></title>
    <url>%2FJava%2FJava8-%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Java8 在 Windows10 中的安装与配置下载与安装 JDK在 Oracle 官网下载 JDK 在 Windows 下的安装包 jdk-8u161-windows-x64.exe 将 jdk-8u161-windows-x64.exe 安装到 D:\ST\JAVA\jdk\route 配置 Java 环境变量右键我的电脑-属性-高级系统设置-环境变量-系统变量 进行如下配置: 12345678# 新建 JAVA_HOMED:\ST\JAVA\jdk\route# 新建 CLASS_PATHD:\ST\JAVA\jdk\route\lib# 添加一条 Path 的值D:\ST\JAVA\jdk\route\bin 验证 Java 安装的有效性在命令行中输入 java -version 验证，出现以下内容说明安装配置成功。 123java version "1.8.0_161"Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode) Java8 在 CentOS7 中的安装与配置下载与解压 JDK在 Oracle 官网下载 JDK 在 Linux 下的安装包 jdk-8u161-linux-x64.tar.gz 在根目录下创建 /abs/software 目录，用 Xftp 将本地下载好的 JDK 包传输到 /abs/software 目录 解压到指定目录 123# 将JDK解压到/abs/app/中tar -zxvf jdk-8u161-linux-x64.tar.gz -C /abs/app/ 配置 Java 环境变量将 JDK 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport JAVA_HOME=/abs/app/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 验证 Java 安装的有效性在命令行中输入 java -version 验证，出现以下内容说明安装配置成功。 123java version "1.8.0_161"Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LNMP 网站无法访问的解决方法]]></title>
    <url>%2F%E8%8A%92%E6%9E%9C%E5%B8%83%E4%B8%81%2FLNMP-%E7%BD%91%E7%AB%99%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[解决方案问题说明之前搭建基于 LNMP + WordPress 的博客，到目前为止出现过两次 Error establishing a database connection 无法访问网站 解决过程// 2017.10.30 在排除了服务器的问题之后 初步推断是 MySQL 数据库连接的问题 查看 MySQL 日志 在 /var/log/mysqld.log 中发现以下 1234567892017-10-30 11:49:45 9260 [ERROR] InnoDB: Cannot allocate memory for the buffer pool2017-10-30 11:49:45 9260 [ERROR] Plugin 'InnoDB' init function returned error.2017-10-30 11:49:45 9260 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.2017-10-30 11:49:45 9260 [ERROR] Unknown/unsupported storage engine: InnoDB2017-10-30 11:49:45 9260 [ERROR] Aborting 根据日志搜索 MySQL 出现 InnoDB: Cannot allocate memory for the buffer pool 参考 解决MySQL : InnoDB: Cannot allocate memory for the buffer pool 有人也遇到了相同的问题 原因是给的内存不够 期间查看和重启 MySQL 都出错，各种方法都解决不了 在 /etc/my.conf 中做以下修改，并重启云主机 1innodb_buffer_pool_size =256M 问题得到了解决。 // 2018.2.10 又出现相同的问题，由于把解决方案记录在博客上了，也没搜到合适的解决方案。 重启了一次云主机和 MySQL 问题得到了解决。 收获网站能正常访问了 删除不必要的插件 做好备份 做好备份 做好备份]]></content>
      <categories>
        <category>芒果布丁</category>
      </categories>
      <tags>
        <tag>LNMP</tag>
        <tag>解决方案</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 伪分布式安装]]></title>
    <url>%2FHadoop%2FHadoop-%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[说明安装环境12345操作系统: CentOS7 mini JDK 安装包: jdk-8u161-linux-x64.tar.gzHadoop 安装包: hadoop-2.6.0-cdh5.7.0.tar.gz 注意事项CentOS7 mini 需要配置双网卡 虚拟机里的 JDK 版本要和物理机的一致，从 Oracle 官网下载。 Hadoop 安装包下载地址，注意版本一致。 前期准备安装虚拟机安装虚拟机 CentOS7 mini，过程略。 下载相关安装包在注意事项中提供了相关资源下载的链接或方法。 验证环境给 CentOS7 mini 配置双网卡 在虚拟机中 ping 物理机地址和 baidu.com ，验证其内外网的连通性。 通过 XShell 5 进行远程连接,方便后面的操作。 安装 JDK上传并解压安装包在 Oracle 官网下载 JDK 在 Linux 下的安装包 jdk-8u161-linux-x64.tar.gz 在根目录下创建 app 目录，用 Xftp 将本地下载好的 JDK 包传输到 app 目录 解压到指定目录 1tar -zxvf jdk-8u161-linux-x64.tar.gz -C /app/ 配置 Java 环境变量配置系统环境变量 1234vi ~/.bash_profileexport JAVA_HOME=/app/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATH 让配置生效1source ~/.bash_profile 在命令行中输入 java 验证。 安装 ssh安装命令1yum install ssh 生成 ssh 的 key12345678910111213# 生成 ssh 的 keyssh-keygen -t rsa用 ls -la 命令找到隐藏文件 .ssh 并 cd 进去# 备份 id_rsa.pubcp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys# 在 .ssh 目录下查看 authorized_keyscat authorized_keys 验证1234567# 免密码登陆ssh localhost# 退出exit Hadoop 环境配置下载、上传、解压 Hadoop 安装包Hadoop 安装包 hadoop-2.6.0-cdh5.7.0.tar.gz 下载地址 用 Xftp 把下载好的安装包上传到 /app 目录下 解压到指定目录 1tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C /app/ 修改配置文件在 /app/hadoop-2.6.0-cdh5.7.0/etc/hadoop 下修改配置文件 hadoop-env.sh 123#注释掉 export JAVA_HOME=$&#123;JAVA_HOME&#125; 并添加如下:export JAVA_HOME=/app/jdk1.8.0_161 core-site.xml 123456789&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://bs1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/app/tmp&lt;/value&gt;&lt;/property&gt; hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; slaves 123vi slaves# 把 localhost 改为你设定的主机名 启动 hdfs格式化 hdfs (PS: 仅第一次执行) 在 bin 目录下执行 1./hdfs namenode -format 在 sbin 目录下执行以下脚本启动 hdfs 1./start-dfs.sh 验证是否启动 hdfs 方法一: 1234567jps# 出现以下三个进程则为正常启动NameNodeDataNodeSecondaryNameNode 方法二: CentOS7 关闭防火墙，出于安全和方便的考虑，这里采用暂时关闭防火墙，实际应用时应设定相应的防火墙规则。 外部物理机应该在 hosts 文件中加入一条 “虚拟机主机名&nbsp;&nbsp;&nbsp;ip 地址”的解析。 12345678910111213141516171819#查看防火墙的状态systemctl status firewalld.service# 开启防火墙systemctl start firewalld.service# 关闭防火墙systemctl stop firewalld.service#开机启用防火墙systemctl enable firewalld.service#开机禁用防火墙systemctl disable firewalld.service 通过在物理机的浏览器地址栏输入以下访问 WebUI1http://虚拟机的主机名:50070 停止 hdfs在 sbin 目录下执行以下脚本停止 hdfs1./stop-dfs.sh 配置 Hadoop 环境变量配置系统环境变量 1234vi ~/.bash_profileexport HADOOP_HOME=/app/hadoop-2.6.0-cdh5.7.0export PATH=$HADOOP_HOME/bin:$PATH 让配置生效1source ~/.bash_profile 在命令行中输入 hadoop 验证。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop安装</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 在 VirtualBox 中的克隆]]></title>
    <url>%2FLinux%2FCentOS7-%E5%9C%A8-VirtualBox-%E4%B8%AD%E7%9A%84%E5%85%8B%E9%9A%86%2F</url>
    <content type="text"><![CDATA[前言克隆 CentOS6 和克隆 CentOS7 过程大致相同，有些细节不一样。 思路在克隆CentOS虚拟机后发现网卡不显示，创建的是完整克隆，主机名和物理地址都一样，这里的想法是克隆出除 ip 地址、物理地址、主机名不一样其他都一样的虚拟机。 克隆根据需求创建链接克隆或完整克隆，这里采用的是完整克隆。 重新分配物理地址删除 /etc/sysconfig/network-scripts/ifcfg-enp0s3 文件中的物理地址 删除 /etc/sysconfig/network-scripts/ifcfg-enp0s8 文件中的物理地址 1删除两行：UUID 和物理地址 删除文件 /etc/udev/rules.d/70-persistent-ipoib.rules 1rm -rf /etc/udev/rules.d/70-persistent-ipoib.rules 修改 ifcfg-enp0s8 文件中的 ip这块网卡设置的是静态 ip，用来远程连接，过程略。 修改主机名CentOS7 修改主机名不同于 CentOS6 用 hostnamectl 命令查看或修改与主机名相关的配置，以下两个主机名设置一致。 123hostnamectl --static set-hostname [主机名]hostnamectl set-hostname [主机名] 手动更新/etc/hosts在文件最后增加一行:1IP 地址 主机名 重启linux重启启动 linux: 1init 6 测试进行内外网是否连通的测试 123ping -c 4 baidu.comping -c 4 192.168.10.x 以上完成了对虚拟机的有效克隆。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>克隆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS6 在 VirtualBox 中的网络配置及克隆]]></title>
    <url>%2FLinux%2FCentOS6%E5%9C%A8VirtualBox%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%85%8B%E9%9A%86%2F</url>
    <content type="text"><![CDATA[前言与 VMware 相比， VirtualBox 安装命令行版的 Linux 操作系统占系统内存会相对较小。 网络配置NAT 模式下自动联网对于 CentOS 来说，配置网卡是一个比较重要的内容。 首先在新建虚拟机的时候我们给网卡配置成 NAT 模式； 登陆之后在命令行下输入 ifconfig 查看网卡的信息； 然后 ping 网关、baidu 等等，确保网络的畅通； 以上是采用 DHCP 获得的 ip 配置静态 IP将 DHCP 自动获取 ip 设置成静态 ip 1vi /etc/sysconfig/network-scripts/ifcfg-eth0 (PS:&nbsp;&nbsp;&nbsp;不同版本的CentOS的网卡名可能存在差异，以实际为主) 设置 ONBOOT=yes 将 dhcp 改为 static 依次添加 ip 地址、子网掩码、网关 123IPADDR=NETMASK=GATEWAY= (PS:&nbsp;&nbsp;&nbsp;以上 IP 设置自己根据外部网络适配器自己指定) 重启网卡1service network restart 测试网络是否畅通 配置 DNS 服务123vi /etc/resolv.confnameserver 8.8.8.8 (PS:&nbsp;&nbsp;&nbsp;nameserver 的地址根据实际情况改动) 以上完成了 CentOS6 网卡的配置 CentOS虚拟机克隆思路 在克隆 CentOS 虚拟机后发现网卡不显示，创建的是完整克隆，主机名和物理地址都一样，这里的想法是克隆出除 ip 地址、物理地址、主机名不一样其他都一样的虚拟机。 克隆根据需求创建链接克隆或完整克隆，这里采用的是完整克隆。 重新分配物理地址删除 /etc/sysconfig/network-scripts/ifcfg-eth0 文件中的物理地址 1删除两行： UUID 和物理地址 删除文件 /etc/udev/rules.d/70-persistent-net.rules 1rm -rf /etc/udev/rules.d/70-persistent-net.rules 修改主机名1vi /etc/sysconfig/network 重启 Linux1init 6 修改 hosts12vi /etc/hosts 文件。在文件最后增加一行 ：IP 地址 主机名 以上完成了对虚拟机的有效克隆]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>网络配置</tag>
        <tag>克隆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 VirtualBox 中为 CentOS7mini 配置双网卡]]></title>
    <url>%2FLinux%2F%E5%9C%A8VirtualBox%E4%B8%AD%E4%B8%BACentOS7mini%E9%85%8D%E7%BD%AE%E5%8F%8C%E7%BD%91%E5%8D%A1%2F</url>
    <content type="text"><![CDATA[配置过程需求分析要同时满足虚拟机访问互联网和远程连接，需要配置两块网卡。 一块为 NAT 网络，这块用来访问互联网。 另一块为 Host-Only 网络，进行远程连接。 注意事项在 Virtualbox 中配置双网卡一定要先只开一个，等配好一个再配下一个，不然第二个不识别。 在全局设定里配置 NAT 网络、 Host-Only 网络 在新建虚拟机时选择网卡(NAT) 配置第一块网卡123456vi /etc/sysconfig/network-scripts/ifcfg-enp0s3改动 ONBOOT=yes重启网卡 service network restart测试能否访问外网 ping –c 4 baidu.com 给 CentOS7mini 装一些常用的指令CentOS7mini 中缺少 vim， wget， curl， lsof， ifconfig之类的指令，需要自己单独安装。执行： 1yum -y install nano vim wget curl net-tools lsof 如果提示出错执行 yum makecache 重建缓存即可 配置第二块网卡关机后添加 Host-Only 网卡，开机复制网卡文件后进行相关配置。 进入网卡文件夹的目录 /etc/sysconfig/network-scripts 后输入 cp ifcfg-enp0s3 ifcfg- enp0s8 ，通过 ls 查看是否添加上了对 ifcfg- enp0s8 作出如下修改： 123456BOOTPROTO=staticNAME=enp0s8DEVICE=enp0s8ONBOOT=yesIPADDR=192.168.10.10NETMASK=255.255.255.0 配置好之后重启网卡，ping 192.168.10.10 验证 Host-Only 网卡的有效性。 至此，在 CentOS7mini 中配置双网卡就完成了，可以根据这个配置作为别的操作的基础。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>VirtualBox</tag>
        <tag>双网卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Different between equals and ==]]></title>
    <url>%2FJava%2FDifferent-between-equals-and%2F</url>
    <content type="text"><![CDATA[equals :&nbsp;&nbsp;&nbsp;判断逻辑上值是否相等== :&nbsp;&nbsp;&nbsp;判断内存引用是否相等 Demo]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>equals</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FHexo%2FHello-World%2F</url>
    <content type="text"><![CDATA[这是一个基于 Hexo + Github Pages + NexT 搭建的静态博客。 在整个过程中遇到过许多问题，从最开始的搭建到后期的主题优化，用 “ 折腾 ” 二字来总结恰好合适。 现在网站外观基本上满足了我的需求，算是一个一般的皮囊吧，我更想让他有一个有趣的灵魂。 事件经过起源源于我对未知事物的好奇心，很久之前的某天在知乎上看到了一篇文章，萌生了搭建个人博客的想法，于是我开始按照他的教程开始了搭建，由于当时水平有限，一开始的尝试并未成功，尝试几次后也就搁置了，当我成功按这个方法搭建好的时候我已经在另一个平台成功搭建了自己的个人博客。 后来看了另一篇文章腾讯云的 1001 种玩法在云主机中搭建博客，注册了域名，租了云主机，基于 LNMP+WordPress 搭建了自己的个人博客，在上面做了一些笔记和更新了一些教程，在这个过程中学到了不少东西，关键是对整个过程有了一些概念为现在本站的搭建有了一些积极的作用。 因为原来的云主机有别的用途，所以根据网上的教程优化了主题，准备将原来的博客我认为有价值的博文迁移过来，在本站继续写。 最初的想法最初的想法很简单，在个人博客记录学习笔记，然后分享给有需要的人，最好是能产生一些沟通交流，还有就是总结我遇到的问题和解决方法。可能是博客的推广工作和搜索优化做的不到位的原因，最初的博客没有太大的访问量，评论更是寥寥无几。现在的想法是完善之前的不足，写出有价值的文章。 搭建及优化的过程搭建基于 Hexo + Github Pages + NexT 的静态博客主要参照的是 教你免费搭建个人博客，Hexo&amp;Github NexT主题优化主要参照的是 打造个性超赞博客Hexo+NexT+GithubPages的超深度优化 (包含怎么用Markdown写文章) / Hexo 折腾记 / Hexo+Next主题优化 遇到问题的解决方法遇到问题要善于运用搜索引擎，想清楚用什么关键词描述你的问题。 提问之前先看看 提问的智慧]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hello world</tag>
      </tags>
  </entry>
</search>
