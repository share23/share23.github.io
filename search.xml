<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ZooKeeper 指南]]></title>
    <url>%2FZooKeeper%2FZooKeeper-%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Zookeeper 分布式协调服务介绍ZooKeeper 简介官方介绍 ZooKeeper 是一种集中式服务，用于维护配置信息，命名，提供分布式同步 和提供组服务。所有这些类型的服务都以分布式应用程序的某种形式使用。 每次实施它们都需要做很多工作来修复不可避免的错误和竞争条件。由于难 以实现这些类型的服务，应用程序最初通常会吝啬它们，这使得它们在变化 的情况下变得脆弱并且难以管理。即使正确完成，这些服务的不同实现也会 在部署应用程序时导致管理复杂性。 通俗理解 Zookeeper 是一个分布式协调服务，就是为用户的分布式应用程序提供协调 服务。只要半数以上的节点存活， Zookeeper 就能正常提供服务。 Zookeeper 在底层管理（存储、读取）用户提交的数据,为数据提供节点 监听的服务。 分布式协调框架分布式框架的好处 可靠性: 一个或几个节点的崩溃不会导致整个集群的崩溃 可伸缩性: 可以通过动态添加主机的方式以及修改少量配置文件，以便提升集群性能 透明性: 隐藏系统的复杂性，对用户体现为一个单一的应用 分布式框架的弊端 竞态条件：一个或多个主机尝试运行一个应用，但是该应用只需要被一个主机所运行 死锁： 两个进程分别等待对方完成 不一致性： 数据的部分丢失 分布协调服务，解决分布式服务在工作时产生的问题 竞态条件（多个主机同时对一个文件进行操作，俗称抢资源） 死锁（多个主机互相等待对方完成） 不一致性（资源文件丢失或者主机宕机） 典型应用场景: Hadoop 高可用的自动容灾 Zookeeper 的作用名字服务 //标识集群中的所有节点，(节点能够向其注册并产生唯一标识) 配置管理 //存储配置文件，以便共享 集群管理 //添加或删除节点同时，实时更新集群信息 选举机制 锁和同步服务 //当文件进行修改，会将其进行加锁，防止多用户同时写入 高有效性数据注册 Zookeeper 集群的安装部署节点说明Zookeeper 安装在 s102、s103、s104上，这三个节点同时是 Hadoop 的 DataNode Zookeeper 本地模式安装配置123456789101112131415161718192021222324# 0.安装配置在s101# 1.解压 Zookeeper 安装包到指定目录 tar -xzvf zookeeper-3.4.10.tar.gz -C /soft/# 2.为 Zookeeper 创建符号链接 ln -s zookeeper-3.4.10/ zk# 3.为 Zookeeper 配置环境变量 sudo nano /etc/profile export ZK_HOME=/soft/zk export PATH=$PATH:$ZK_HOME/bin# 4.使环境变量生效 source /etc/profile# 5.将 zk/conf/zoo_sample.cfg 重命名或复制一份命名为 zoo.cfg mv zoo_sample.cfg zoo.cfg cp zoo_sample.cfg zoo.cfg# 6.启动 Zookeeper zkServer.sh start Zookeeper 完全分布式安装配置脚本 xcall.sh 脚本 xsync.sh 脚本位置 12345678910111213141516171819202122232425262728293031# 1.修改 zoo.cfg 文件,指定工作目录， dataDir=/home/centos/zookeeper server.102=s102:2888:3888 server.103=s103:2888:3888 server.104=s104:2888:3888# 2.将 Zookeeper 文件夹同步到其他节点 xsync.sh /soft/zookeeper-3.4.10 xsync.sh /soft/zk# 3.使用 root 权限，将环境变量同步到其他节点 su root xsync.sh /etc/profile exit# 4.在 s102-s104 中创建文件夹 /home/centos/zookeeper，并在文件夹内创建文件 myid xcall.sh "mkdir /home/centos/zookeeper" ssh s102 "echo 102 &gt; /home/centos/zookeeper/myid" ssh s103 "echo 103 &gt; /home/centos/zookeeper/myid" ssh s104 "echo 104 &gt; /home/centos/zookeeper/myid"# 5.按顺序启动 s102-s104 的 Zookeeper,分别在 s102、s103、s104 上执行 source /etc/profile; zkServer.sh start# 5.1 开启远程主机 Zookeeper 客户端 zkCli.sh -server s102:2181# 6.查看状态，可以看出 leader、follower zkServer.sh status 配置 Hadoop 高可用的自动容灾123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# 0.关闭 Hadoop stop-all.sh# 1.修改 hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;# 2.修改 core-site.xml &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;s102:2181,s103:2181,s104:2181&lt;/value&gt; &lt;/property&gt;# 3.分发配置文件 xsync.sh /soft/hadoop/etc/hadoop/hdfs-site.xml xsync.sh /soft/hadoop/etc/hadoop/core-site.xml# 4.初始化zk hdfs zkfc -formatZK# 5.启动 HDFS start-dfs.sh# 6.启动 Zookeeper 命令行脚本 zkCli.sh zkCli.sh# 7.正常配置启动的进程,通过 xcall.sh jps 查看==================== s101 jps ===================1782 Jps1418 NameNode1725 DFSZKFailoverController==================== s102 jps ===================1185 DataNode1288 JournalNode1368 Jps1067 QuorumPeerMain==================== s103 jps ===================1158 DataNode1063 QuorumPeerMain1261 JournalNode1341 Jps==================== s104 jps ===================1141 DataNode1321 Jps1244 JournalNode1053 QuorumPeerMain==================== s105 jps ===================1056 NameNode1235 Jps1164 DFSZKFailoverController# 8.通过关闭 s101 的 NameNode 进程验证Hadoop 高可用的自动容灾 通过网页可以看出 s105 的状态为 active ，实现了自动容灾# 9.查看 Hadoop 高可用文件 [zk: s102:2181(CONNECTED) 2] ls /hadoop-ha/mycluster [ActiveBreadCrumb, ActiveStandbyElectorLock] [zk: s102:2181(CONNECTED) 3] get /hadoop-ha/mycluster/ActiveBreadCrumb myclusternn1s101 [zk: s102:2181(CONNECTED) 4] get /hadoop-ha/mycluster/ActiveStandbyElectorLock myclusternn1s101 # ActiveStandbyElectorLock是临时结点，负责存储active状态下的节点地址# ActiveBreadCrumb是永久结点，负责在zk会话关闭时，下一次启动状态下正确分配active节点，避免脑裂（brain-split）,即两个active节点状态 Zookeeper 数据结构、命令Zookeeper 数据结构Zookeeper 特性： Zookeeper 文件系统以 / 为根目录，文件系统为树形结构，每一个目录我们称之为结点 Zookeeper 不存文件，只存数据 每个结点(znode)存放的数据最多1M Zookeeper 命令1234567891011121314151617181920212223242526272829303132# 进入 Zookeeper 客户端zkCli.sh# 查看帮助help # 列出根结点下的子结点ls /# 查看结点上存储的数据get /# 获取指定节点的状态(无数据)stat# 在根结点写入数据tomset / 'tom' # 创建有数据结点create /a 'tomas'# 创建无数据结点create /b ''# 删除没有子结点的结点delete /b# 递归删除结点rmr /a# 退出客户端quit Zookeeper API 的使用Demo 编写 xzk.sh 脚本1234567#!/bin/bash for((i=102 ; i&lt;=104; i++)) ; do tput setaf 2 echo ==================== s$i $1 =================== tput setaf 9 ssh s$i "source /etc/profile ; zkServer.sh $1" done 编写 xzk.sh 脚本，是为了方便在 s101 节点上启动所有的 Zookeeper 进程 可执行的命令为: xzk.sh start xzk.sh stop xzk.sh status Zookeeper 的原理以及选举机制Zookeeper 的读写操作 读操作：所有zk节点都可以提供读请求(包括follower和leader) 写操作：需要先通过leader节点，leader同意之后，可以向指定节点写入 Zookeeper 的选举机制Zookeeper 虽然在配置文件中并没有指定 master 和 slave，但是 Zookeeper 工作时，是有一个节点为 leader，其他则为 follower Leader是通过内部的选举机制临时产生的 Zxid: ZooKeeper 的事务 id，每次处理请求（读或写），此 id 会 +1 myid: 在配置时指定的 Zookeeper id 初始化选举 3个节点组成的 ZooKeeper 集群，myid 分别为102、103、104 启动 s102 ，选举状态为 looking 状态， 启动 s103 ，和 s102 进行通信,交换选举结果，根据 myid 的大小确认 s103 为 leader 启动 s104 ，和 s102 、s103 进行通信,交换选举结果，虽然 s104的 myid 的大于 s103 ，但是 s103 获得了一半以上的选票，s103 还是 leader 非全新集群选举机制 适用于 ZooKeeper 节点故障之后的重新选举 先比较 Zxid ，Zxid 大的胜出 Zxid 相同的情况下， myid 大的胜出]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 基础篇]]></title>
    <url>%2FHive%2FHive-%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Hive 介绍Hive 简介官方介绍 Apache Hive™ 数据仓库软件有助于使用 SQL 读取、编写和管理驻留在分布式存储中的大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和 JDBC 驱动程序以将用户连接到 Hive。 通俗理解 Hive 就是在 Hadoop 上架了一层 SQL 接口，可以将 SQL 翻译成 MapReduce 去 Hadoop 上执行，这样就使得数据开发和分析人员很方便的使用 SQL 来完成海量数据的统计和分析，而不必使用编程语言开 MapReduce 那么麻烦。 Hive 安装与配置待补充。。。 启动 Hive 的顺序 启动 ZooKeeper 1xzk.sh start xzk.sh 脚本内容 启动 Hadoop(HDFS+MR) 1start-all.sh 启动 Hive 1hive DDL 操作### Hive 基本概念 Hive 应用场景。 Hive 与hadoop的关系。 Hive 与传统数据库对比。 Hive 的数据存储机制。 Hive 基本操作 Hive 中的DDL操作。 在Hive 中如何实现高效的JOIN查询。 Hive 的内置函数应用。 Hive shell的高级使用方式。 Hive 常用参数配置。 Hive 自定义函数和Transform的使用技巧。 Hive UDF/UDAF开发实例。 Hive 执行过程分析及优化策略]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 的序列化]]></title>
    <url>%2FHadoop%2FHadoop-%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[序列化序列化与反序列化的概念序列化 是指将结构化对象转化成字节流在网上传输或写到磁盘进行永久存储的过程 反序列化 是指将字节流转回结构化对象的逆过程 序列化的应用序列化用于分布式数据处理的两大领域 1. 进程间通信 2. 永久存储 序列化的格式要求1. 紧凑：体积小，节省带宽 2. 快速：序列化过程快速 3. 可扩展：新API支持旧数据格式 4. 支持互操作：跨语言 Writable 接口说明Hadoop 使用的序列化格式为 Writable Writable 接口定义了两个方法 1. write 将对象写入 DataOutput 二进制流 2. readFields 从 DataInput 二进制流读取对象 Writable 接口实现类Writable 接口实现类包含以下 1. int 对应的 Writable 为 IntWritable 2. Long 对应的 Writable 为 LongWritable 3. String 对应的 Writable 为 Text 以 IntWritable 为例，在阅读源码之后发现 可以直接通过 new 的方式直接带参创建实例化对象 也可以调用空参构造创建实例化对象之后通过 set 方法赋值 IntWritable 案例使用 IntWritable 实现 Hadoop 的序列化与反序列化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import org.apache.hadoop.io.IntWritable;import org.junit.Test;import java.io.*;/** * @user: share * @date: 2018/7/28 * @description: 测试Hadoop的序列化与反序列化 */public class TestHadoopSerial &#123; /** * 单元测试Hadoop的序列化 * @throws IOException */ @Test public void testSerial() throws IOException &#123; //创建IntWritable对象 IntWritable iw = new IntWritable(66); //创建输出流对象 DataOutputStream dos = new DataOutputStream(new FileOutputStream("e:/e/haddop.h")); //iw将值写入输出流dos iw.write(dos); //关闭输出流 dos.close(); &#125; /** * 单元测试Hadoop的反序列化 * @throws IOException */ @Test public void testDeserial() throws IOException &#123; //创建输入流对象 DataInputStream dis = new DataInputStream(new FileInputStream("e:/e/haddop.h")); //创建IntWritable对象 IntWritable iw = new IntWritable(); //iw读取输入流dis的值 iw.readFields(dis); //得到iw中的值 int i = iw.get(); //输出i System.out.println(i); //关闭输入流 dis.close(); &#125;&#125; 自定义 PersonWriteable自定义Person类 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.io.Serializable;/** * @user: share * @date: 2018/7/28 * @description: 自定义Person类 */public class Person implements Serializable &#123; private String name; private int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "Person&#123;" + "name='" + name + '\'' + ", age=" + age + '&#125;'; &#125;&#125; 自定义 PersonWriteable 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import org.apache.hadoop.io.Writable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * @user: share * @date: 2018/7/28 * @description: 自定义PersonWriteable实现Person的序列化与反序列化 */public class PersonWriteable implements Writable &#123; //定义person private Person person; //设置get方法 public Person getPerson() &#123; return person; &#125; //设置set方法 public void setPerson(Person person) &#123; this.person = person; &#125; /** * 重写序列化方法 * @param out * @throws IOException */ public void write(DataOutput out) throws IOException &#123; //序列化name字段 out.writeUTF(person.getName()); //序列化age字段 out.writeInt(person.getAge()); &#125; /** * 重写反序列化方法 * @param in * @throws IOException */ public void readFields(DataInput in) throws IOException &#123; //初始化person person = new Person(); //反序列化name字段 person.setName(in.readUTF()); //反序列化age字段 person.setAge(in.readInt()); &#125;&#125; Person的序列化测试类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import org.junit.Test;import java.io.*;/** * @user: share * @date: 2018/7/28 * @description: 测试Person的序列化与反序列化 */public class TestPersonSerial &#123; /** * 单元测试Person的序列化 * @throws IOException */ @Test public void testPersonSerial() throws IOException &#123; //新建Person对象 Person p = new Person("sam", 20); //创建PersonWriteable对象 PersonWriteable pw = new PersonWriteable(); //调用set方法赋值 pw.setPerson(p); //创建输出流对象 DataOutputStream dos = new DataOutputStream(new FileOutputStream("e:/e/person.j")); //pw将值写入输出流dos pw.write(dos); //关闭输出流 dos.close(); &#125; /** * 单元测试Person的反序列化 * @throws IOException */ @Test public void testPersonDeserial() throws IOException &#123; //创建PersonWriteable对象 PersonWriteable pw = new PersonWriteable(); //创建输出流对象 DataInputStream dis = new DataInputStream(new FileInputStream("e:/e/person.j")); //读取输入流中的对象 pw.readFields(dis); //得到Person对象 Person p = pw.getPerson(); //输出Person System.out.println(p.toString()); //关闭输入流 dis.close(); &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Serialization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell 脚本编程指南]]></title>
    <url>%2FLinux%2FShell-%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[说明Shell 和 Shell 脚本的区别Shell 指的是一种应用程序，常见的如 Windows 下的 CMD 窗口和 Linux 中的命令行窗口 Shell 脚本指的是为 Shell 编写的脚本程序 使用场景因为在 Linux 集群下有配置与管理的需求，工作量比较大 所以使用 Shell 脚本处理同步配置文件、管理每一个节点等操作 属性Shell 脚本编写流程 创建文件并添加执行权限 创建以 .sh 为后缀结尾的脚本文件 123456# 通过 vi 或 vim 创建文件,只需要在创建完成并添加内容之后保存即可。 vi shell1.sh vim shell2.sh # 通过 touch 创建文件，可直接创建。 touch shell3.sh 为脚本添加执行权限 文件权限分为三种，分别为可读、可写、可执行(r w x),分别为 4 2 1 用户权限分为三种，分别为当前用户、同组用户、其他用户(u g o) 12345# 为当前用户赋予执行权限 chmod u+x shell1.sh# 为当前用户、同组用户、其他用户赋予执行权限 chmod a+x shell2.sh 编写脚本头，声明解释器 格式如下: 12# 在第一行开头编写` #!/bin/bash 编写脚本内容 略 添加注释，以 # 作为标识符 写注释一方面规范代码，增强可阅读性 在方便自己理解的同时也有利于整理思路 强制命令解析12345678# 在我们输入 echo pwd 的时候并不能显示当前所在目录的绝对路径，只是将 pwd 原样输出# 如果想将当前目录原样输出可以 echo `pwd` 实现强制命令解析# 强制命令解析符号为 ``# 在 Shell 脚本编写时经常使用 获取变量的方式1234567# 在命令行和Shell脚本中正确获取变量的方式 echo $PATH echo $&#123;PATH&#125; echo "$PATH"# 在命令行和Shell脚本中正错误取变量的方式 echo '$PATH' 参数的提取12345678910111213141516# 和脚本配合使用，作为传入的参数# 参数的个数 $# # 第 n 个参数 $n# 当前脚本（命令）名称 $0# 取出所有参数 $@# 参数左移 shift 方法控制变色12345# 设置前景色 tput setaf ColorNumber # 设置背景色 tput setab ColorNumber Shell 中比较运算符1234567891011121314151617# 相等 equalif [ $i -eq 30 ]# 不等 not equal if [ $i -ne 30 ]# 大于 greater thanif [ $i -gt 30 ]# 小于 less thanif [ $i -lt 30 ]# 大于等于 greater equalif [ $i -ge 30 ]# 小于等于 less equalif [ $i -le 30 ] if 判断语句if 判断语句格式 1if [ $i -lt | -gt | -eq | -ne | -ge | -le 10 ] ; then command ; elif .... then ... ; else .... ; fi 小例子 12345678#!/bin/bash# 判断输入的参数 $1 的值小于30的话输出 young# 判断输入的参数 $1 的值小于50的话输出 middle# 判断输入的参数 $1 的值大于或等于50的话输出 old# $1 为第一个参数if [ $1 -lt 30 ] ; then echo young $# $@; elif [ $1 -lt 50 ] ; then echo middle ; else echo old ; fi for 循环for 循环语句格式 12# for 循环语句格式for (( exp1; exp2; exp3 )); do COMMANDS; done 循环打印 1-9 12#!/bin/bashfor (( i=0; i&lt;10; i++ )); do echo $i ; done 循环打印 9X9 乘法表 12345678910#!/bin/bashfor (( i=1; i&lt;10; i++ )); do for (( j=1; j&lt;=i; j++ )); do #tput setaf $j # -ne 为不换行和转义 echo -ne "$j"x"$i"=$((j*i))'\t' done tput setaf $i echo done 123456for NAME [in WORDS ... ] ; do COMMANDS; done# 通过对1.txt中的单词进行分割，取出所有单词并进行打印#!/bin/bashfor x in `cat 1.txt` ; do echo $x ; done while 循环while 循环语句格式 12# while 循环语句格式while COMMANDS; do COMMANDS; done while循环实现输出 1-9： 123456#!/bin/bashi=1while (( $i&lt;10 )); do echo $i i=$((i+1))done while循环实现99乘法表： 1234567891011#!/bin/bashi=1while (( $i&lt;10 )); do j=1 while (( $j&lt;=$i )); do echo -ne $&#123;j&#125;x$&#123;i&#125;=$(( j*i ))'\t' j=$((j+1)) done i=$((i+1)) echodone case 语句case 语句格式 12# case 语句格式case WORD in [PATTERN [| PATTERN]...) COMMANDS ;;]... esac 小例子 123456#!/bin/bashcase $1 in helloworld | tom ) echo 1 ;; hello ) echo 2 ;; * ) echo 3 ;;esac Demo自定义脚本的前提在编写脚本完成之后先确定其权限,再将其上传到 /usr/local/bin 目录中 完成以上操作保证了在任意目录执行自定义脚本 xcall 批量操作脚本前提是为每个节点安装和配置好ssh，使主节点能远程登陆其他节点 使用格式为: xcall.sh command 例如: xcall.sh yum install -y rsync 12345678910111213#!/bin/bash# for循环for((i=1 ; i&lt;=5; i++)) ; do # 更改文本颜色 tput setaf 2 # 输出以下文本 echo ==================== hadoop00$i $@ =================== # 更改文本颜色 tput setaf 4 # ssh 远程登陆主机 hadoop$i ,执行输入的参数的命令 ssh hadoop00$i $@done xsync 同步脚本从主节点同步配置文件到子节点 使用格式为: xsync.sh config_filename 例如: xsync.sh /app/software/hadoop/etc 1234567891011121314151617181920#!/bin/bash# 指出当前用户名name=`whoami`# 指定文件所在文件夹名称dir=`dirname $1`# 指定文件的文件名filename=`basename $1`# 进入到dir中cd $dir# 得到当前目录的绝对路径fullpath=`pwd`for((i=2 ; i&lt;=5; i++)) ; do tput setaf 2 echo ==================== hadoop00$i $@ =================== tput setaf 9 # 远程同步命令 l 保留软连接 r 递归文件夹 rsync -lr $filename "$name"@hadoop00"$i":$fullpathdone]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于大数据的餐饮推荐系统总结]]></title>
    <url>%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A4%90%E9%A5%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[介绍推荐系统不单单指某一方面的技术，而是作为一个完整的系统而存在，要考虑到很多方面才能做出一个有价值的推荐系统。 在信息过载的时代，有太多的信息被产生，推荐系统的作用是让人们在海量的信息中查看到更多有效的信息。 推荐系统的价值在生活中的许多方面都有体现，浏览器、社交软件、购物软件等中都能看到其身影，为用户提供个性化推荐，在引导用户浏览商品信息的同时带动消费。 大数据时代，数据就是一种资源，为了资源能有效的使用，我们需要在数据中通过一定的方式得到有价值的信息。 通过大数据技术搭建平台，基于平台进行推荐业务代码的编写，将大数据技术与推荐系统结合在一起，可以带来更高的价值。通过基于大数据的推荐系统处理海量的数据得出有价值的推荐结果。 说明最初是想学习大数据相关技术，然后从事和大数据相关的职业，所以在毕业设计选题时选择了《基于大数据的餐饮推荐系统设计》。 真正在查了很多资料才发现可能想的有点简单了，从零开始突然发现自己面对的是一个庞然大物的时候是不知所措的。 在和朋友的交流中的出一个结论：先做一个最简单的，再慢慢优化。 在一步一步的试错中前行。 过程技术的选型最初的设想是要实现离线推荐与实时推荐，最终选定了 Lambda 架构。 Lambda 架构的主要思想是将大数据系统构建为多个层次。 这里分为三层: 批处理层、实时处理层、服务层。 批处理层: 负责处理离线数据产生离线推荐信息 实时处理层: 负责处理实时数据产生实时推荐信息 服务层: 实现与用户的交互，将推荐数据信息可视化展示给用户 数据不同于图书与音乐之类的数据，餐饮数据并不好量化。 以音乐为例，可以很清晰的以歌曲名称、歌手、热度、类型等来实现量化。 而餐饮数据充满着不确定性，一种食物每家饭店可能都做得不一样。 我觉得最好状态的是为每个用户创建画像，并为每种食物建立详细描述的标签，通过二者的关联度进行相关推荐。 技术无止境，但都是服务于业务，以上的想法要实现还有很长的路要走。 美团和饿了么都未提供经过脱敏的餐饮数据，也没有相关的支持。 所以数据只能自己造，使用 Python 写了一个小的餐饮评分数据生成器，通过其产生数据作为数据来源。 离线数据产生了10万条，实时数据是通过 Linux 的定时执行工具 crontab 定时执行 Python 脚本。 首先要将离线评分数据写入 HBase 数据库，先把本地的数据导入到 HDFS 中，在将其从 HDFS 中传输到 HBase 数据库中。 大数据平台的搭建参照 Spark Streaming 实时流处理项目实战环境搭建汇总完成大数据平台的搭建。 推荐代码的编写通过业务代码实现对餐饮的推荐，最终选用了 Spark MLlib 中的 ALS 算法。 考虑过基于用户或基于商品的算法进行相关推荐，自己生成的餐饮数据的数量与特征与以上不是很匹配，就选了个基于评分数据的协同过滤算法。 相对来说这是比较简单的一种方式。 如何在搭建好并经过连通性测试的大数据平台上进行业务代码的编写，也就是整合 ALS。 22基于Spark机器学习跟实时流计算的智能推荐系统 给了我很大的启发。 首先是在思想上的，这篇文章让我对 ALS 的原理与整个推荐系统的流程有了初步的认识。 其次是将他的核心业务代码移植到我的毕业设计中去，虽然有些代码的细节具体用法不是很懂，但是大致上的意思能理解个大概。 计划在学习了 Scala 之后把注释写完整，思路再理一遍。 作者的可视化界面使用 ASP.NET 编写的，出于节省内存的目的我这里采用的 Spring Boot框架整合 Thymeleaf模板。 与作者相比，我在搭建大数据平台的时候用到了更多的大数据组件，但是作者在三年前就已经完成了业务代码的编写，在网上搜相关问题都能看到他的提问。 系统流程图 项目地址项目地址 不足评分数据缺乏真实的来源经过实际测试，没有真实的数据来源得到的推荐结果并没有实际价值。 也无法得出合适的参数优化模型。 没有冷启动由于一切都是基于用户对餐饮数据的评分而产生的推荐结果，新的用户并未产生评分数据。 缺乏评分也就无法实现推荐，像我们现在注册账户刚登进去就会给我们很多选项，让我们选择自己感兴趣的。 这些都是实现了冷启动，通过这些确定了我们的爱好偏向，保证了在缺乏足够的数据之前进行相关推荐。 推荐算法单一不同于传统推荐系统，现阶段的推荐系统是一个综合性质的应用，已经不再满足使用单一推荐算法完成推荐，融合众多推荐算法发挥他们的优势完成推荐才能使推荐系统更好地为人们服务。 本设计采用的是机器学习库中的ALS算法，在之后的研究与学习中还应该融合其他优秀的推荐算法，基于实际需求充分发挥每个推荐算法的优势，搭建出更优秀的推荐系统。 硬件限制受限于电脑内存、电脑性能和机器数量，要同时兼顾平台的搭建、大数据各组件的运行和代码的开发运行，在大数据推荐平台的搭建上选择伪分布式安装并未用到集群，不能完整地发挥大数据的优势。 在代码编写完成后的运行速度与效率也受到限制，规模与真正的应用存在差距。在以后的工作中希望可以在这些方面得到更好的锻炼 总结最终虽然完成了推荐系统的设计，但是也发现了许多问题与不足。 整体过程中遇到了很多坑，一直在踩坑与爬坑。 挺感谢这段经历的，在这个过程中收获了很多。 下一阶段的目标是把基础打牢，然后跟着官网更深入地学习大数据。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 学习笔记]]></title>
    <url>%2FJava%2FSpring-Boot-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[介绍Spring Boot 是什么待补充。。。 Spring Boot 能做什么待补充。。。 选择 Spring Boot 的初衷// 2018.4.15 因为有个项目需要用一个页面来显示从 HBase 中读取的数据 这只是一个想法 未验证其可行性 // 2018.4.23 已验证其可行性 现已实现从 HBase 中读取的数据然后显示在网页中 实现将其热部署 实例解析需求分析有个项目需要动态展示 HBase 中的数据 HBase 中的数据是会变化的，所以项目需要热部署，并对页面动态刷新。 环境说明开发工具: Intellij IDEA Java版本: 1.8.0_161 项目目录结构1234567891011121314151617181920212223242526├── src│ ├── main│ │ ├── java│ │ │ ├── foodrecommender│ │ │ ├── controller│ │ │ │ └── RecController.java│ │ │ ├── dao│ │ │ │ └── FoodRecDAO.java│ │ │ ├── domain│ │ │ │ └── FoodRec.java│ │ │ ├── utils│ │ │ │ └── HBaseUtils.java│ │ │ │ │ │ │ └── FoodrecomenderApplication.java│ │ ││ │ └── resources│ │ ├── static│ │ │ ├── css│ │ │ └── images│ │ ├── templates│ │ │ └── index.html│ │ └── application.yml │ ││ └── test │├── pom.xml 学习历程 Step 1: 运行第一个 Spring Boot 项目 根据慕课网上的教程 2小时学会Spring Boot 解锁怎么在 IDEA 新建一个 Spring Boot 项目 并写一个简易的 Hello Spring Boot 显示在网页中 Step 2: Spring Boot 集成 Thymeleaf 一般都是先在 pom.xml 文件中添加 Thymeleaf 的依赖如下 12345&lt;!--Thymeleaf模板--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 在 templates 目录中新建 index.html (PS: 为了显示的效果，我直接放进一个做好布局静态的 html文件) 采用 ModelAndView 在 RecController.java 中指定 index.html 进行相关测试，能在页面中看到效果。 Step 3: 测试是否能从 HBase 中读取数据 写一个 HBase 操作工具类 HBaseUtils.java 验证是否能从 HBase 中读取数据 Step 3: 规范代码结构 实体类 FoodRec.java 数据访问层 FoodRecDAO.java Step 4: 将从 HBase 中读取的数据显示在前端的 index.html 中 参考 Spring Boot和Thymeleaf集成 中 ModelAndView 传值的例子如下 1234567@RequestMapping(method = RequestMethod.GET)public ModelAndView home(ModelAndView model) &#123; model.getModel().put("firstName", "set param firstname in ModelAndView"); model.getModel().put("lastName", "set param lastname in ModelAndView"); model.setViewName("RequestParam"); return model;&#125; 参考链接如下: https://www.cnblogs.com/han-1034683568/p/7520012.html https://github.com/Terry-Shi/blog/wiki/Spring-Boot-thymeleaf#spring-boot%E5%92%8Cthymeleaf%E9%9B%86%E6%88%90 https://segmentfault.com/a/1190000014352023 Step 5: 实现热部署 由于 HBase 中的数据是变化的，所以需要动态更新页面 而当前的网站是静态的，只是在启动的时候执行了一次 参考 springboot集成thymeleaf(不重启刷新html) spring-boot 速成(2) devtools之热部署及LiveReload 他教程中最后那步针对的是 MAC 系统 如果是 Windows 系统可以参考第二个链接中的说明 步骤为 Keymap -&gt; Other -&gt; Maintenance 我找到之后发现他的快捷键为 Ctrl + Alt + Shift + / Step 5: 验证热部署 在 HBase 中对数据做相关的覆盖操作，发现网站页面上的数据并没有改变 在手动刷新网页后能得到新的结果 但这不能满足我们的需求 根据实际需求在网上找到这篇文章 网页页面 自动刷新的3种代码 采用其中最简单的第一种方法，实现了页面的自动刷新 项目展示 项目地址待上传 总结从 0 开始到得到我想要的效果。 一个小目标一个小目标的实现，期间经历了多次可行性验证 一开始有很多问题，都借助搜索引擎解决了 参照着 提问的智慧 中的一原则，也向别人请教了一些问题 大多数都是自己解决的 记录下整个过程 这就是我走过的路]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenStack 安装环境配置指南]]></title>
    <url>%2F%E4%BA%91%E8%AE%A1%E7%AE%97%2FOpenStack-%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[// 2018.4.7 更新，由于云主机面临到期，陆续将文章转移到本博客，纪念一下原博客(www.sudos.cn) // 2017.8.26 原文 Overview前言 由于篇幅原因，这里只写出了 OpenStack 安装环境的搭建。 我用的是在 VirtualBox 里安装 CentOS7 mini ，用 CentOS7 mini 配置 OpenStack 的 Ocata 版。 配置前提请参考以下链接 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在VirtualBox中为CentOS7mini配置双网卡 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CentOS7 在 VirtualBox 中的克隆 OpenStack官方安装文档 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OpenStack Installation Tutorial for Red Hat Enterprise Linux and CentOS OpenStack配置教程 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;手把手教你安装OpenStack——Ocata安装指南（上） 对于初学者来说当你完成了这一步后，你就可以参照相关链接进行下一步的配置。 官方文档很重要，做好备份，遇到问题上网查。 不是搭出来就好了，重要的是在这个搭建的过程中锻炼了你解决问题的能力，以及搭建完成后你对整个流程的理解和后续 OpenStack 的学习、应用。 介绍 这里采用的是 OpenStack Ocata 版本示例框架至少需要两个节点（主机）来启动基本 虚拟机或实例。块存储和对象存储等可选服务需要额外的节点。 示例架构来配置控制器节点和一个计算节点以下最低要求应支持具有核心服务和多个CirrOS实例的概念验证环境： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;控制器节点：1个处理器，4 GB内存和5 GB存储 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算节点：1处理器，2 GB内存和10 GB存储 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虚拟机为 VirtualBox&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;操作系统为 CentOS7 mini Install and Configuration安全设置 自动创建安全密码 1openssl rand -hex 10 在之后的配置中记录好以下密码 网络 网卡 192.168.10.0/24 网关192.168.10.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;controllernode1：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enp0s3 dhcp &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; enp0s8 192.168.10.11 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;computenode1： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enp0s3 dhcp &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; enp0s8 192.168.10.12 关闭防火墙 12345678# 网络配好之后关闭防火墙关闭防火墙 systemctl stop firewalld.service# 禁止防火墙开机启动systemctl disable firewalld.service# 运行完成后查看防火墙的状态Service firewalld status 关闭 selinux 1234567891011# 打开selinux配置文件vi /etc/selinux/config# 关闭selinuxSELINUX=disabled# setenforce 0，使配置立即生效setenforce 0# 查看selinux的状态service selinux status 配置 hosts 地址解析 123456# 打开hosts配置文件vi /etc/hosts# 添加以下host解析记录192.168.10.11 controllernode1192.168.10.12 computenode1 配置网络接口 123456789# 验证与外网连通性,分别在控制节点和计算节点上：ping -c 4 www.baidu.com# 验证控制节点和计算节点的连通性：# 控制节点上输入ping -c 4 computenode1# 计算节点上输入ping -c 4 controllernode1 NTP 服务 安装相关包 1yum install chrony contorllernode1 做以下操作 1234567891011121314# 打开chrony配置文件vi /etc/chrony.conf# 进行相关配置server NTP_SERVER iburst (s1b.time.edu.cn)# NTP服务器地址 http://blog.csdn.net/hello_hwc/article/details/44748785# 可以根据需要将NTP_SERVER替换为合适的NTP服务器，建议不用改。然后添加以下，即允许计算节点同步。（计算节点IP网段属于192.168.10.0）allow 192.168.10.0/24# 启动服务。systemctl enable chronyd.servicesystemctl start chronyd.service computenode1 做以下操作 123456789101112# 打开chrony配置文件vi /etc/chrony.conf# 可以将其他的几个同步地址注释掉。server controllernode1 iburst# 同步控制节点。systemctl enable chronyd.servicesystemctl start chronyd.service# 启动服务（如果发现同步的不是控制节点，那么重启一下NTP服务即可。)systemctl restart chronyd.service 验证操作 1234567891011# 在控制节点上同步时间chronyc sources# 带星号的是NTP当前同步的地址# 计算节点上同步chronyc sources# 结果显示如下MS Name/IP address Stratum Poll Reach LastRx Last sample==================================================^* controllernode1 4 6 377 116 -24.8s[ -1433s] +/- 13.6s 安装 OpenStack 包以下操作在所有节点上进行 123456789101112# 启用OpenStack库yum install centos-release-openstack-ocata# 在所有节点上升级包#If the upgrade process includes a new kernel, reboot your host to activate it.yum upgrade# 安装OpenStack客户端yum install python-openstackclient# CentOS默认启用了SELinux，安装openstack-selinux来自动管理OpenStack服务的安全策略yum install openstack-selinux SQL 数据库数据库一般运行在控制节点上 安装并配置组件 12345678910111213141516171819# 安装相关包yum install mariadb mariadb-server python2-PyMySQL# 创建并编辑/etc/my.cnf.d/openstack.cnf 文件，并完成以下操作。在配置文件中添加以下配置： # 其中bind-address为控制节点IP地址。 [mysqld]bind-address=192.168.10.11default-storage-engine=innodbinnodb_file_per_table=onmax_connections=4096collation-server=utf8_general_cicharacter-set-server=utf8# 启动数据库并设置开机启动systemctl enable mariadb.servicesystemctl start mariadb.service# 运行mysql_secure_installation脚本来保证数据库安全，为root账户设置一个合适的密码mysql_secure_installationRoot password for the database 消息队列OpenStack 使用消息队列来协调服务之间的状态和操作，消息队列服务一般运行在控制节点上。OpenStack 支持 RabbitMQ ，Qpid 以及 ZeroMQ 等消息队列服务。本指南使用 RabbitMQ 消息队列服务。 12345678910111213# 安装相关包yum install rabbitmq-server# 启动消息队列并设置开机启动systemctl enable rabbitmq-server.servicesystemctl start rabbitmq-server.service# 添加openstack用户# 使用合适的密码替换掉 RABBIT_PASSrabbitmqctl add_user openstack RABBIT_PASS# 允许openstack用户的配置、写、读权限rabbitmqctl set_permissions openstack ".*" ".*" ".*" 缓存令牌认证服务缓存令牌认证服务的认证机制使用 Memcached 来缓存令牌，一般运行在控制节点上。 123456789101112# 安装相关包yum install memcached python-memcached# 打开memcached配置文件vi /etc/sysconfig/memcached# 配置IP地址，将127.0.0.1改为控制节点IPOPTIONS="-l 192.168.10.11,::1"# 启动 Memcached服务并设置开机启动systemctl enable memcached.servicesystemctl start memcached.service PS至此完成 OpenStack 的安装环境的搭建，注意检查是否在安装的过程中出现错误，每一步都做好备份。 后续的各项服务安装参照官方文档安装就可以了，遇到有疑问的看看别人的安装教程，善用搜索工具。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>Guide</tag>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA 使用指南]]></title>
    <url>%2FJava%2FIDEA-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[增加使用体验新建类后自动添加自定义的注释 在主界面使用快捷键 Ctrl + Alt + S 进入 Settings 页面 依次打开 Editor --&gt; File and Code Templates --&gt; Files --&gt; Class 为 Class 自动添加注释，修改如下面文本注释部分 1234567/** * @user: $&#123;USER&#125; * @date: $&#123;DATE&#125; * @description: */public class $&#123;NAME&#125; &#123;&#125; 为 Scala Class Scala Object自动添加注释，修改如下 123456789101112131415# Scala Class/** * Created by $&#123;USER&#125; on $&#123;DATE&#125;. */class $&#123;NAME&#125; &#123;&#125;# Scala Object/** * Created by $&#123;USER&#125; on $&#123;DATE&#125;. */object $&#123;NAME&#125; &#123;&#125; 问题合集IDEA 通过 Maven 导入的依赖包下面存在红色波浪线问题描述: 创建的 Maven Project 在添加相关依赖后自动下载，自动添加的依赖包的下面存在红色波浪线，在使用过程中存在问题，Reimport 之后还是没能解决。 在 Maven 的仓库中删除重新下载或是更改 Maven 的镜像源重新下载都没解决这个问题。 解决方案: 通过搜索引擎找到了一个解决方案 在确定已经下载好相关依赖包的情况下，在 pom.xml 中删掉相关的 dependency 之后 Reimport Maven 然后重新添加相关的 dependency 再 Reimport Maven ，红色波浪线消失，然后相关的包可以正常使用。 (PS: 相关的 dependency 为存在红色波浪线的依赖包) 将项目从 IDEA pull 到 GitHub 出错2018.6.23 问题描述: 将本地项目 pull 到 GitHub 报错 错误内容如下所示: 1234510:36 Can't finish GitHub sharing process Successfully created project 'Food_Recommender' on GitHub, but initial push failed: unable to access 'https://github.com/share23/Food_Recommender.git/': OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:44310:48 Push failed: Failed with error: unable to access 'https://github.com/share23/Food_Recommender.git/': SSL certificate problem: self signed certificate in certificate chain 解决思路: 从报错内容初步推断是 SSL 的错误，搜索问题日志中的关键语句，寻求解决方案。 解决方案: 参考 执行Git命令时出现各种 SSL certificate problem 的解决办法 在项目的根目录使用 Git Bash 在里面输入 git config --global http.sslVerify false 完成以上操作之后再次 pull 项目。 IDEA 忽略大小写//2018.7.26 问题描述: IDEA 在使用过程中使用小写不会自动提示原因是默认设置了大小写敏感 解决方案: 在主界面使用快捷键 Ctrl + Alt + S 进入 Settings 页面 依次打开 Editor --&gt; General --&gt; Code Completion --&gt; Case sensitive completion 将 Case sensitive completion 设为 None]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>Guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 日志生成器]]></title>
    <url>%2FPython%2FPython-%E6%97%A5%E5%BF%97%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[需求分析网站日志需求分析由于缺乏真实网站日志，在这里用 Python 2.7.5 构建日志生成器模拟网站日志，作为之后实验的基础。 之后的实验为用 Flume 采集网站的日志信息，基于此做一系列的处理。 为搭建好网站之后采集真实网站日志信息做准备。 数据集需求分析由于缺乏有效的 ALS 数据集做为模型的训练与测试，在这里用 Python 2.7.5 构建日志生成器模拟数据集。 为 Spark Streaming + ALS 推荐系统的模型的训练与测试做准备 环境说明系统: CentOS 7.2 mini Python 版本: 2.7.5 (PS: Python 为 CentOS 7.2 mini 自带的,可通过 python --version 来查看 Python 的版本，注意 Python 2.X 和 Python 3.X 存在很大的区别，这里采用的是系统自带的 Python 2.7.5) Python 日志生成器的编写网站日志生成器的编写格式可根据实际需求做出调整 generate_log.py 如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#coding=UTF-8import randomimport time#从什么搜索引擎跳转过来的http_referers = [ "http://www.baidu.com/s?wd=&#123;query&#125;", "https://www.google.com/search?q=&#123;query&#125;", "https://www.sougou.com/web?query=&#123;query&#125;", "http://cn.bing.com/search?q=&#123;query&#125;", "https://search.yahoo.com/search?p=&#123;query&#125;"]#搜索关键字search_keyword = [ "鸡排饭", "虾排饭", "芒果布丁", "海鲜自助", "水果拼盘", "黄焖鸡米饭", "西红柿牛腩面"]#网站链接路径url_path = [ "class/111.html", "class/112.html", "class/113.html", "class/114.html", "class/115.html", "class/116.html", "anyway/666", "NoBUG/666"]#ip地址ip_slices = [10,123,132,125,168,187,25,66,37,168,131,86,53,19,163]#状态码status_codes = ["200","404","500"]#sample(seq, n)从序列seq中选择n个随机且独立的元素#随机生成一个带refer和keyword的urldef sample_refer(): if random.uniform(0, 1) &gt; 0.2: return "_" refer_str = random.sample(http_referers, 1) query_str = random.sample(search_keyword, 1) return refer_str[0].format(query=query_str[0])#随机生成一个urldef sample_url(): return random.sample(url_path,1)[0]#随机生成一个ipdef sample_ip(): slice = random.sample(ip_slices,4) return ".".join([str(item) for item in slice])#随机生成一个状态码def sample_status_code(): return random.sample(status_codes,1)[0]def generate_log(count = 10): #获取本地时间 time_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()) #打开access.log文件,并赋予 f 写的权限 #w+为消除文件内容，然后以读写方式打开文件。 #a+为以读写方式打开文件，并把文件指针移到文件尾 #为了将数据连续的写入文件采用 a+ #f = open("/abs/project/data/log/access.log","w+") f = open("/abs/project/data/log/access.log","a+") while count &gt;=1: query_log="&#123;ip&#125;\t&#123;local_time&#125;\t\"GET /&#123;url&#125; HTTP/1.1\"\t&#123;status_code&#125;\t&#123;refer&#125;".format(local_time=time_str,url=sample_url(),ip=sample_ip(),refer=sample_refer(),status_code=sample_status_code()) #测试时输出到控制台，在实际使用中是将他存入相应的文件中 #print query_log #将日志输入到access.log f.write(query_log + "\n") count = count - 1if __name__ == '__main__': generate_log() 数据集生成器的编写我们所需要的数据集的格式为: &nbsp;&nbsp;user_id::food_id::food_rating 可根据实际需求做出调整 generate_dataset.py 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#coding=UTF-8import random#食物idfood_id = [1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030]#用户iduser_id = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]#用户对食物的评价food_rating = [1,2,3,4,5]#sample(seq, n)从序列seq中选择n个随机且独立的元素#随机生成一个fooddef sample_food(): return random.sample(food_id, 1)[0]#随机生成一个用户def sample_user(): return random.sample(user_id, 1)[0]#随机生成一个食物评价def sample_foodrating(): return random.sample(food_rating, 1)[0]def generate_dataset(count = 10): #打开rating文件,并赋予 f 写的权限 #w+为消除文件内容，然后以读写方式打开文件。 #a+为以读写方式打开文件，并把文件指针移到文件尾 #为了将数据连续的写入文件采用 a+ #f = open("/abs/project/data/dataset/rating","w+") f = open("/abs/project/data/dataset/rating","a+") while count &gt;=1: #query_log="&#123;food&#125;\t&#123;user&#125;\t&#123;foodrating&#125;".format(food=sample_food(),user=sample_user(),foodrating=sample_foodrating()) query_log="&#123;food&#125;::&#123;user&#125;::&#123;foodrating&#125;".format(food=sample_food(),user=sample_user(),foodrating=sample_foodrating()) #测试时输出到控制台，在实际使用中是将他存入相应的文件中 #print query_log #将日志输入到rating f.write(query_log + "\n") count = count - 1if __name__ == '__main__': generate_dataset() 扩展定时执行工具 Linux &nbsp;&nbsp; crontab &nbsp;&nbsp;网站:&nbsp;&nbsp;http://tool.lu/crontab &nbsp;&nbsp;参照介绍可以得出每隔1分钟执行一次的 crontab 表达式&nbsp;&nbsp; */1**** 在 /abs/project 目录中新建日志生成器脚本 log_generator.sh 1234# 新建日志生成器脚本 log_generator.shvi log_generator.shpython /abs/lib/generate_log.py 新打开一个界面，然后查看输出 tail -200f /abs/project/data/log/access.log 为 log_generator.sh 增加可执行权限 chmod u+x log_generator.sh 执行脚本 ./log_generator.sh 配置 crontab 1234567891011# 配置 crontabcrontab -e# 每隔1分钟执行一次 log_generator.sh*/1 * * * * /abs/project/log_generator.sh# 查看 crontabcrontab -l# 删除 crontabcrontab -r 小结学会了用 Python 实现日志生成器的代码编写，并在此基础上进行相关拓展 学会了周期性执行任务的守护进程 crontab 的基本操作与使用]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume Bug Solutions]]></title>
    <url>%2FFlume%2FFlume-Bug-Solutions%2F</url>
    <content type="text"><![CDATA[环境相关错误telnet localhost 44444 出现 Connection refused在 Flume 实战练习一 中有用过 telnet localhost 44444 来验证 NetCat Source，当时没出现问题 在进行将服务器的信息采集到本地控制台输出的实验中 telnet localhost 44444 出现 Connection refused 问题思考: 1.防火墙的问题2.telnet的问题3.代码的问题 解决思路: 1.关闭防火墙再次验证问题未得到解决2.重装telnet并参照网上相关教程进行配置仍未解决问题3.重新检查了一遍代码还是未解决问题 再次通过搜索引擎根据关键字进行相关检索 检索到以下有价值的信息 Which source port can I use to test an event with my flume agent? 根据里面 telnet should be performed at port 44444 only as the Flume source is listening at that port. 的回答得出了应该先启动 Flume Agent ，让其监听 44444 端口 ,才能运行 telnet localhost 44444 完成以后的操作 之前的 Connection refused 应该是 Flume Agent 的启动出现错误 重新检查配置文件与启动命令，最终解决了这个问题。]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
        <tag>Solution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7.2 安装 MySQL]]></title>
    <url>%2FLinux%2FCentOS7-2-%E5%AE%89%E8%A3%85-MySQL%2F</url>
    <content type="text"><![CDATA[前言安装的 MySQL 版本为 5.7.21 在 CentOS7.2 中安装 MySQL 是作为别的实验的基础。 比如将结果写入数据库和网站的搭建。 本文参照&nbsp;&nbsp;&nbsp;centos7.2安装MySQL&nbsp;&amp;&amp;&nbsp;CentOS 7 下 Yum 安装 MySQL 5.7结合实际环境进行相关操作 验证是否已经安装 MySQL1234567891011# 检查是否安装了mysqlrpm -qa | grep mysql# 检查是否安装了mariadbrpm -qa | grep mariadb # 一般使用此命令即可卸载成功rpm -e xxx # 卸载不成功时使用此命令强制卸载)rpm -e --nodeps xxx 安装 MySQL12345678# 下载 MySQL 源wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm# 安装 MySQL 源yum -y install mysql57-community-release-el7-10.noarch.rpm# 安装 MySQLyum -y install mysql-community-server 重启 MySQL12# 重启 MySQL 服务systemctl restart mysqld 生成临时密码12345678# 生成临时密码grep "password" /var/log/mysqld.log# 生成后的效果显示如下2018-03-14T04:05:03.080507Z 1 [Note] A temporary password is generated for root@localhost: T&lt;HTR#6Gngds# 临时密码为T&lt;HTR#6Gngds 登陆 MySQL 和修改 root 用户密码12345678910# 登陆 MySQL mysql -uroot -pEnter password: 上面生成的临时密码# 修改 root 用户密码,新密码XXXXXXXXXX为8位以上包含大小写字母、数字和符号mysql&gt; alter user 'root'@'localhost' identified by 'XXXXXXXXXX';Query OK, 0 rows affected (0.00 sec)# 重新登陆 MySQLmysql -uroot -pXXXXXXXXXX 允许远程访问123456# 赋予 root 用户远程访问权限mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'XXXXXXXXXX' WITH GRANT OPTION;# 防火墙配置firewall-cmd --permanent --add-port=3306/tcpfirewall-cmd --reload 配置默认编码为 utf812345vi /etc/my.cnf# 在[mysqld]下添加如下键值对character_set_server=utf8init_connect='SET NAMES utf8' 重启 MySQL12#重启 MySQL 使配置生效systemctl restart mysqld 扩展一现在有一个实验需要将统计结果写入数据库中，所以数据库做以下操作 123456789101112131415161718192021222324# 创建数据库 share_sparkcreate database share_spark;# 使用数据库 share_sparkuse share_spark;# 新建一张表 wordcountcreate table wordcount(word varchar(50) default null,wordcount int(10) default null);# 查看数据库中包含的表mysql&gt; show tables;+-----------------------+| Tables_in_share_spark |+-----------------------+| wordcount |+-----------------------+1 row in set (0.00 sec)# 查看表 wordcount 的详情mysql&gt; select * from wordcount;Empty set (0.02 sec) //Demo 待上传 Demo]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 实时流处理项目实战环境搭建汇总]]></title>
    <url>%2FSpark%2FSpark-Streaming-%E5%AE%9E%E6%97%B6%E6%B5%81%E5%A4%84%E7%90%86%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[说明这里做的汇总旨在归纳总结怎么从0开始搭建 Spark Streaming 实时流处理项目实战环境 之前已经模块化的把各组件的安装配置总结出来了 现在是补充说明并把他们整合在一起 在此基础上进行相关操作与拓展 前提虚拟机软件: Virtual Box 服务器端: CentOS 7.2 (配置双网卡及其他基本配置) 客户端: Windows 10 + IDEA + Maven 3.3.9 JDK 的安装这里安装的 JDK 版本为 1.8.0_161 参照 Java8 的安装 在服务器端和客户端进行安装 在 CentOS 7.2 和 Windows 10 中安装要保证版本的一致性 避免出现不必要的错误 Scala 的安装这里安装的 Scala 版本为 2.11.8 参照 Scala 的安装 在 CentOS 7.2 中安装 在 Windows 10 IDEA 中安装 Scala 插件 1Configure --&gt; Plugins --&gt; Install JetBrains plugins --&gt; 搜索Scala --&gt; Install 如果插件下载失败，参考 Intellij idea安装scala插件详解 进行相关操作 Maven 的安装这里安装的 Maven 版本为 3.3.9 参照 Maven3.3.9 的安装在服务器端和客户端进行安装 在 CentOS 7.2 和 Windows 10 中安装要保证版本的一致性 避免出现不必要的错误 Hadoop 的安装这里搭建的 Hadoop 版本为 2.6.0-cdh5.7.0 采用的是伪分布式安装 参照 Hadoop 环境搭建 在 CentOS 7.2 中安装 ZooKeeper 的安装这里安装的 ZooKeeper 版本为 3.4.5-cdh5.7.0 参照 ZooKeeper 的安装 在 CentOS 7.2 中安装 HBase 的安装这里安装的 HBase 版本为 1.2.0-cdh5.7.0 参照 HBase 的安装 在 CentOS 7.2 中安装 Spark 的安装这里安装的 Spark 为编译之后的 编译过程在下面有详细介绍 参照 Spark 环境搭建 在 CentOS 7.2 中安装 IDEA + Maven + Spark Streaming主要是基于之前的操作在项目 pom.xml 中添加对应的依赖 参照 Spark Streaming 开发环境搭建 在 IDEA 中进行相关设置 Demo//待上传 Demo]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>BD</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 开发环境搭建]]></title>
    <url>%2FSpark%2FSpark-Streaming-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[说明主要是基于之前的操作在 pom.xml 中添加对应的依赖 前提 安装配置好 IDEA 在 Windows10 中安装配置好 Maven3.3.9 添加依赖在pom.xml 中添加依赖如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;kafka.version&gt;0.9.0.0&lt;/kafka.version&gt; &lt;spark.version&gt;2.2.0&lt;/spark.version&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;hbase.version&gt;1.2.0-cdh5.7.0&lt;/hbase.version&gt; &lt;/properties&gt; &lt;!-- 添加cloudera的repository --&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!-- Scala 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Kafka 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;kafka.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;$&#123;kafka.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Hadoop 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- HBase 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark Streaming 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 以上完成了使用 IDEA 整合 Maven 搭建 Spark Streaming 开发环境]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>BD</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 环境搭建]]></title>
    <url>%2FSpark%2FSpark-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[说明Spark 开发环境的搭建是基于 基于 CentOS7.2 环境编译 Spark-2.2.0 源码 请详细参照以上进行编译步骤进行相关操作 解压 Spark 安装包解压编译好的 Spark 安装包 1tar -zxvf spark-2.2.0-bin-2.6.0-cdh5.7.0.tgz -C /abs/app/ 配置 Spark 环境变量配置系统环境变量 1234vi ~/.bash_profileexport SPARK_HOME=/abs/app/spark-2.2.0-bin-2.6.0-cdh5.7.0export PATH=$SPARK_HOME/bin:$PATH 让配置生效1source ~/.bash_profile 验证在 /abs/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/bin 目录中输入 ./spark-shell --help 查看帮助选项 以本地模式运行 ./spark-shell --master local[2] 可以看到以下 123456789101112131415Spark context Web UI available at http://192.168.10.100:4040Spark context available as 'sc' (master = local[2], app id = local-1520521812619).Spark session available as 'spark'.Welcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ '_/ /___/ .__/\_,_/_/ /_/\_\ version 2.2.0 /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_161)Type in expressions to have them evaluated.Type :help for more information.scala&gt; 在关闭防火墙的情况下通过在物理机的浏览器地址栏输入以下访问 WebUI1http://虚拟机的主机名:4040 以上说明 Spark 安装配置成功]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>BD</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 的安装]]></title>
    <url>%2FHBase%2FHBase-%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[说明先启动 ZooKeeper ，再启动 Hadoop 相关组件才能启动 HBase 下载与解压 HBaseHBase 1.2.0 下载地址 找到并下载 hbase-1.2.0-cdh5.7.0.tar.gz 将他传输到 /abs/software 目录 解压到指定目录 1234567891011121314#将HBase解压到/abs/app/中tar -zxvf hbase-1.2.0-cdh5.7.0.tar.gz -C /abs/app/``` ## 配置 HBase 环境变量将 HBase 目录添加到系统环境变量(~/.bash_profile)中``` bashvi ~/.bash_profileexport HBASE_HOME=/abs/app/hbase-1.2.0-cdh5.7.0export PATH=$HBASE_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 修改配置文件在 /abs/app/hbase-1.2.0-cdh5.7.0/conf 目录下修改配置文件 hbase-env.sh 12345# 导入JAVA_HOME路径export JAVA_HOME=/abs/app/jdk1.8.0_161# 将 export HBASE_MANAGES_ZK=true 改为 falseexport HBASE_MANAGES_ZK=false hbase-site.xml 1234567891011121314&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop:8020/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop:2181&lt;/value&gt;&lt;/property&gt; regionservers 1234vi regionservers# 把 localhost 改为你设定的主机名hadoop 验证启动 ZooKeeper在 /abs/app/zookeeper-3.4.5-cdh5.7.0/bin 目录输入 ./zkServer.sh start 启动 ZooKeeper 在命令行输入 jps 12499 QuorumPeerMain 可以看到 QuorumPeerMain 进程已经启动 启动 HDFS 和 YARN在 /abs/app/hadoop-2.6.0-cdh5.7.0/sbin 目录下执行以下脚本启动 hdfs 1./start-dfs.sh 在命令行输入 jps 12342499 QuorumPeerMain3285 NameNode3374 DataNode3549 SecondaryNameNode 可以看到 NameNode 、 DataNode 、SecondaryNameNode 进程已经启动 在 /abs/app/hadoop-2.6.0-cdh5.7.0/sbin 目录执行以下脚本启动 yarn 1./start-yarn.sh 在命令行输入 jps 1234563699 ResourceManager3789 NodeManager2499 QuorumPeerMain3285 NameNode3374 DataNode3549 SecondaryNameNode 可以看到 ResourceManager 、 NodeManager 进程已经启动 启动 HBase在 /abs/app/hbase-1.2.0-cdh5.7.0/bin 目录中输入 ./start-hbase.sh 启动 HBase 命令行出现以下说明启动成功: 1234starting master, logging to /abs/app/hbase-1.2.0-cdh5.7.0/logs/hbase-root-master-hadoop.outJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0hadoop: starting regionserver, logging to /abs/app/hbase-1.2.0-cdh5.7.0/bin/../logs/hbase-root-regionserver-hadoop.out 在命令行输入 jps 123456785169 HRegionServer5037 HMaster2499 QuorumPeerMain3285 NameNode3374 DataNode3549 SecondaryNameNode3699 ResourceManager3789 NodeManager 可以看到 HRegionServer 、 HMaster 进程已经启动 在关闭防火墙的前提下通过在物理机的浏览器地址栏输入以下访问 WebUI 1http://虚拟机的主机名:60010 测试12345678910111213141516171819202122232425262728293031323334353637# 在 /abs/app/hbase-1.2.0-cdh5.7.0/bin 目录中输入以下命令启动 HBase shell./hbase shell# 查看版本信息hbase(main):002:0&gt; version1.2.0-cdh5.7.0, rUnknown, Wed Mar 23 11:46:29 PDT 2016# 查看当前状态hbase(main):003:0&gt; status1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load# 创建一张表hbase(main):001:0&gt; create 'member','info','address'0 row(s) in 2.7780 seconds=&gt; Hbase::Table - member# 查看HBase中所有的表hbase(main):002:0&gt; listTABLE member 1 row(s) in 0.0420 seconds=&gt; ["member"]# 查看指定表的信息hbase(main):003:0&gt; describe 'member'Table member is ENABLEDmemberCOLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; 'address', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'&#125; &#123;NAME =&gt; 'info', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'&#125; 2 row(s) in 0.1810 seconds 可以在 HBase 的 WebUI 中看到创建的表的信息 * 在 HBase Shell 中输错命令的解决方法为按住 Ctrl + 删除键(backspace) 即可删除]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 环境搭建]]></title>
    <url>%2FHadoop%2FHadoop-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[说明搭建的是伪分布式的 Hadoop 开发环境 安装 JDK搭建之前请确保已经安装了 Java8 的环境 参考 Java8 的安装 安装 ssh安装命令1yum install ssh 生成 ssh 的 key12345678910111213# 生成 ssh 的 keyssh-keygen -t rsa用 ls -la 命令找到隐藏文件 .ssh 并 cd 进去# 备份 id_rsa.pubcp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys# 在 .ssh 目录下查看 authorized_keyscat authorized_keys 验证1234567# 免密码登陆ssh localhost# 退出exit Hadoop 环境配置下载、上传、解压 Hadoop 安装包Hadoop 安装包 hadoop-2.6.0-cdh5.7.0.tar.gz 下载地址 用 Xftp 把下载好的安装包上传到 /abs/software 目录下 解压到指定目录 1tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C /abs/app/ 配置 Hadoop 环境变量配置系统环境变量 1234vi ~/.bash_profileexport HADOOP_HOME=/abs/app/hadoop-2.6.0-cdh5.7.0export PATH=$HADOOP_HOME/bin:$PATH 让配置生效1source ~/.bash_profile 在命令行中输入 hadoop 验证。 配置 HDFS修改配置文件在 /abs/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop 下修改配置文件 hadoop-env.sh 123#注释掉 export JAVA_HOME=$&#123;JAVA_HOME&#125; 并添加如下:export JAVA_HOME=/abs/app/jdk1.8.0_161 core-site.xml 123456789&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/abs/app/tmp&lt;/value&gt;&lt;/property&gt; hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; slaves 1234vi slaves# 把 localhost 改为你设定的主机名hadoop 启动 HDFS格式化 hdfs (PS: 仅第一次执行) 在 /abs/app/hadoop-2.6.0-cdh5.7.0/bin 目录下执行 1./hdfs namenode -format 在 /abs/app/hadoop-2.6.0-cdh5.7.0/sbin 目录下执行以下脚本启动 hdfs 1./start-dfs.sh 验证是否启动 HDFS方法一: 1234567jps# 出现以下三个进程则为正常启动NameNodeDataNodeSecondaryNameNode 方法二: CentOS7 关闭防火墙，出于安全和方便的考虑，这里采用暂时关闭防火墙，实际应用时应设定相应的防火墙规则。 外部物理机应该在 hosts 文件中加入一条 “虚拟机主机名&nbsp;&nbsp;&nbsp;ip 地址”的解析。 1234567891011121314#查看防火墙的状态systemctl status firewalld.service# 开启防火墙systemctl start firewalld.service# 关闭防火墙systemctl stop firewalld.service#开机启用防火墙systemctl enable firewalld.service#开机禁用防火墙systemctl disable firewalld.service 通过在物理机的浏览器地址栏输入以下访问 WebUI1http://虚拟机的主机名:50070 停止 HDFS在 sbin 目录下执行以下脚本停止 hdfs1./stop-dfs.sh 配置 YARN修改配置文件在 /abs/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop 目录下拷贝 mapred-site.xml 1cp mapred-site.xml.template mapred-site.xml 修改 mapred-site.xml 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 修改 yarn-site.xml 1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 启动 YARN在 /abs/app/hadoop-2.6.0-cdh5.7.0/sbin 目录执行以下脚本启动 yarn 1./start-yarn.sh 验证是否启动 YARN方法一: 123456jps# 出现以下两个进程则为正常启动ResourceManagerNodeManager 方法二: CentOS7 关闭防火墙，出于安全和方便的考虑，这里采用暂时关闭防火墙，实际应用时应设定相应的防火墙规则。 外部物理机应该在 hosts 文件中加入一条 “虚拟机主机名&nbsp;&nbsp;&nbsp;ip 地址”的解析。 1234567891011121314#查看防火墙的状态systemctl status firewalld.service# 开启防火墙systemctl start firewalld.service# 关闭防火墙systemctl stop firewalld.service#开机启用防火墙systemctl enable firewalld.service#开机禁用防火墙systemctl disable firewalld.service 通过在物理机的浏览器地址栏输入以下访问 WebUI1http://虚拟机的主机名:8088 停止 YARN在 sbin 目录下执行以下脚本停止 yarn1./stop-yarn.sh 测试测试 HDFS1234567891011121314# 查看根目录下有什么hadoop fs -ls /# 创建目录hadoop fs -mkdir /data# 传入一个文件hadoop fs -put anaconda-ks.cfg /data/# 验证是否传输成功hadoop fs -ls /data# 查看传输内容hadoop fs -text /data/anaconda-ks.cfg 测试 YARN1234# 在 /abs/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce 目录中输入以下命令hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 2 3# 得出 Estimated value of Pi is 4.00000000000000000000 以上完成了 Hadoop 开发环境的搭建]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>BD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 的安装]]></title>
    <url>%2FScala%2FScala-%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装要求安装 Scala 之前先安装 Java 参考&nbsp;&nbsp;Java8 的安装 下载 Scala 与解压Scala 2.11.8 下载地址 找到并下载 scala-2.11.8.tgz 将他传输到 /abs/software 目录 解压到指定目录 1234567891011121314#将Scala解压到/abs/app/中tar -zxvf scala-2.11.8.tgz -C /abs/app/``` ## 配置 Scala 环境变量将 Scala 目录添加到系统环境变量(~/.bash_profile)中``` bashvi ~/.bash_profileexport SCALA_HOME=/abs/app/scala-2.11.8export PATH=$SCALA_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 验证在命令行输入 scala 出现以下说明安装配置成功 1234Welcome to Scala 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_161).Type in expressions for evaluation. Or try :help.scala&gt;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记本换屏日志]]></title>
    <url>%2F%E8%8A%92%E6%9E%9C%E5%B8%83%E4%B8%81%2F%E7%AC%94%E8%AE%B0%E6%9C%AC%E6%8D%A2%E5%B1%8F%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[问题说明在使用过程中笔记本电脑屏幕突然闪烁，然后屏幕横向出现少许线条。 转动笔记本屏幕转轴，可以正常使用。 过了一段时间屏幕出现全屏花屏 然后出现白屏 严重影响使用体验 问题分析在网上查看了相关问题 归结为以下三点: 1.显卡出现问题2.排线磨损老化3.屏幕问题 解决方案手残党还是找靠谱的售后吧！ 手残党还是找靠谱的售后吧！ 手残党还是找靠谱的售后吧！ 去附近的售后结果不尽人意，看着一点都不专业，一上来就说没货，然后等总部发货，连笔记本屏幕那块的转轴都不会拆，联想到上次去那里买固态硬盘谈好价格后还来隔空涨价。 自己动手，丰衣足食。 采用排除法 验证显卡的问题把笔记本外接显示器，一切正常，故排除显卡问题 验证排线的问题从网上买 屏幕排线 、 转轴壳 ， 网卡扣 。 说明 1.首先要会拆机，不会拆机还是去找售后，注意是找靠谱的售后2.屏幕排线是验证是否是屏幕排线的问题的3.转轴壳是第一次拆机时暴力拆除时弄坏了，应该左右移动拆掉4.网卡扣是断掉了，然后在网上搜和你相同的型号的买 换上新的屏幕排线，确定排线的接口都插好了无松动，结果还是花屏 修好了网卡扣和转轴壳 验证屏幕的问题在接触屏幕的时候发现捏住屏幕右下角(看不到的那个模块)屏幕恢复正常。 应该是硬件问题，那一块的芯片接触不良或是质量问题。 虽然得出了问题出在哪里却无法解决，因为不会解决硬件芯片内部的问题。 网上买了一块对应型号的笔记本屏幕，装上问题得到解决。 一定要拍下屏幕后面的型号，因为有的屏幕是30针接口有的是40针接口的。 最终效果 总结 手残党请去找靠谱的售后 在拆机过程中注意事项要谨记 一定要查相关型号的电脑及配件的正确操作方法 请好好对待陪伴你的电脑 遇到问题解决好做好记录]]></content>
      <categories>
        <category>芒果布丁</category>
      </categories>
      <tags>
        <tag>Solution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 实战练习]]></title>
    <url>%2FKafka%2FKafka-%E5%AE%9E%E6%88%98%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前期准备了解 Kafka 架构及核心组件待补充 学习使用 Kafka详情见Kafka 环境部署、Kafka API 编程练习 实战练习实战结构图 概述: 文字描述待补充 需求分析需求：将 A 服务器上的日志实时采集到 B 服务器供 Kafka 消费 根据需求可以采用以下方案实现： Agent A 选型: exec source + memory channel + avro sinkAgent B 选型: avro source + memory channel + kafka sink 写配置文件在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录中新建 avro-memory-kafka.conf 参照 Flume 1.6.0文档 对 avro-memory-kafka.conf 进行相关配置 1234567891011121314151617181920212223242526# avro-memory-kafka.conf: A realtime Flume configuration# Name the components on this agentavro-memory-kafka.sources = avro-sourceavro-memory-kafka.sinks = kafka-sinkavro-memory-kafka.channels = memory-channel# Describe/configure the sourceavro-memory-kafka.sources.avro-source.type = avroavro-memory-kafka.sources.avro-source.bind = hadoopavro-memory-kafka.sources.avro-source.port = 44444# Describe the sinkavro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSinkavro-memory-kafka.sinks.kafka-sink.brokerList = hadoop:9092avro-memory-kafka.sinks.kafka-sink.topic = hello_topicavro-memory-kafka.sinks.kafka-sink.batchSize = 5avro-memory-kafka.sinks.kafka-sink.requiredAcks = 1avro-memory-kafka.sinks.kafka-sink.avro-memory-kafka.sinks.kafka-sink.# Use a channel which buffers events in memoryavro-memory-kafka.channels.memory-channel.type = memory# Bind the source and sink to the channelavro-memory-kafka.sources.avro-source.channels = memory-channelavro-memory-kafka.sinks.kafka-sink.channel = memory-channel 启动相关组件启动 ZooKeeper在 /abs/app/zookeeper-3.4.5-cdh5.7.0/bin 目录输入 ./zkServer.sh start 启动 kafka在 /abs/app/kafka_2.11-0.9.0.0/bin 目录中进行如下操作: 1kafka-server-start.sh $KAFKA_HOME/config/server.properties 启动 agent先启动 avro-memory-kafka: 1flume-ng agent -n avro-memory-kafka -c $FLUME_HOME/conf -f $FLUME_HOME/conf/avro-memory-kafka.conf -Dflume.root.logger=INFO,console exec-memory-avro 在之前 Flume 实战练习 笔记中已经创建 再启动 exec-memory-avro: 1flume-ng agent -n exec-memory-avro -c $FLUME_HOME/conf -f $FLUME_HOME/conf/exec-memory-avro.conf -Dflume.root.logger=INFO,console 验证jps -m 123452595 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server.properties2807 Jps -m2760 Application -n exec-memory-avro -f /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf/exec-memory-avro.conf2526 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2703 Application -n avro-memory-kafka -f /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf/avro-memory-kafka.conf 消费消息 1kafka-console-consumer.sh --zookeeper hadoop:2181 --topic hello_topic 在 /abs/data 目录中进行以下操作: 12345echo hellospark &gt;&gt; data.logecho hellospark2 &gt;&gt; data.logecho hellospark3 &gt;&gt; data.logecho hellospark4 &gt;&gt; data.logecho hellospark5 &gt;&gt; data.log 可以在消费端看到刚才输入的内容]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Flume</tag>
        <tag>Practise</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo d 失败的解决方法]]></title>
    <url>%2FHexo%2Fhexo-d-%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[解决过程环境说明客户端平台:&nbsp;&nbsp;Windows10 博客托管平台:&nbsp;&nbsp;GitHub 问题描述在用 hexo d 更新 Hexo 静态博客时，报了以下错误 123FATAL bash: /dev/tty: No such device or addresserror: failed to execute prompt script (exit code 1)fatal: could not read Username for 'https://github.com': Invalid argument 验证问题所在验证本地文件是否出错 1234# 启动本地服务器 hexo server# 在浏览器输入 http://localhost:4000/ 经过以上操作发现本地文件一切正常 与 GitHub 的连接问题 从报的错中可以推断出差不多是与 GitHub 的连接问题 解决方案通过搜索引擎查得到相应的解决方案 归纳如下: 1.重新生成 SSH key 12345# 在CMD中输入以下命令ssh-keygen -t rsa -C "Github的注册邮箱地址"# 一直按Enter知道出现以下Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 用 Sublime 打开 /c/Users/user/.ssh/id_rsa.pub 并复制其内容 访问 GitHub – Settings – SSH and GPG keys – New SSH key Title:&nbsp;&nbsp;blog &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Key:&nbsp;&nbsp;id_rsa.pub 的内容 2.修改站点配置文件 _config.yml 123456# 把deploy中的repo由注释的格式改为以下格式deploy: type: git #repo: https://github.com/username/username.github.io.git repo: git@github.com:username/username.github.io.git branch: master 验证1234# 在CMD中依次输入以下命令，结果不存在报错，Hexo博客能正常更新hexo cleanhexo ghexo d]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Solution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka API 编程练习]]></title>
    <url>%2FKafka%2FKafka-API-%E7%BC%96%E7%A8%8B%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[IDEA + Maven 构建开发环境安装与配置 IDEA 参照 INTELLIJ IDEA 教程 进行 IDEA 的安装与配置 安装与配置 Maven 参照 INTELLIJ IDEA MAVEN 配置&nbsp;&nbsp;与&nbsp;&nbsp;Maven3.3.9 的安装 进行 Maven 在 Windows10 下的安装与整合 IDEA 的配置 新建 Maven Project 打开 IDEA 然后 Create New Project 进行如下设置: 进行相关配置新建的 Maven Project 有报错，在右下角的 Event Log 中点击 Import Changes 对 pom.xml 作以下修改: 12345678910111213141516171819202122232425 &lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;kafka.version&gt;0.9.0.0&lt;/kafka.version&gt; &lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--Kafka 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;kafka.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;$&#123;kafka.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Kafka 的源码是用 Scala 编写的，我们如果要用 Java 来编程需要做如下设置 将目录结构设置为 Producer API 的使用新建 Java 类参照目录结构在 com.share.spark.kafka 包中新建 KafkaProperties 类 123456789101112131415161718package com.share.spark.kafka;/** * Kafka常用配置文件 */public class KafkaProperties &#123; //定义开发过程中要用到的常量 //定义ZooKeeper public static final String ZK = "192.168.10.100:2181"; //定义Topic，hello_topic是之前在CentOS7中创建好的 public static final String TOPIC = "hello_topic"; //定义broker_list，broker是Kafka配置文件中配置好的 public static final String BROKER_LIST = "192.168.10.100:9092"; &#125; 参照目录结构在 com.share.spark.kafka 包中新建 KafkaProducer 类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.share.spark.kafka;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;import java.util.Properties;/** * Kafka生产者 */public class KafkaProducer extends Thread&#123; private String topic; private Producer &lt;Integer, String&gt; producer; public KafkaProducer(String topic)&#123; this.topic = topic; Properties properties = new Properties(); //broker.list properties.put("metadata.broker.list",KafkaProperties.BROKER_LIST); //序列化 properties.put("serializer.class","kafka.serializer.StringEncoder"); /*握手机制，发送消息是否需要服务器的反馈 0为生产者不等待leader的握手机制 1为leader写信息在本地日志中并立刻响应握手机制 -1为leader将等待所有副本的握手机制 */ properties.put("request.required.acks","1"); producer = new Producer&lt;Integer, String&gt;(new ProducerConfig(properties)); &#125; @Override public void run() &#123; int messageNo = 1; while(true) &#123; String message = "message_" + messageNo; producer.send(new KeyedMessage&lt;Integer, String&gt;(topic,message)); System.out.println("Sent :" +message); messageNo ++; try&#123; Thread.sleep(2000); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 参照目录结构在 com.share.spark.kafka 包中新建 KafkaClientApp 类 12345678910package com.share.spark.kafka;/** * Kafka API 测试 */public class KafkaClientApp &#123; public static void main(String[] args) &#123; new KafkaProducer(KafkaProperties.TOPIC).start(); &#125;&#125; 项目代码: Demo 准备与测试说明: 服务器为 CentOS7,本地为 Windows10 本地访问服务器需要对服务器的防火墙进行相关设置(这里采用临时关闭防火墙) 12345# 查看防火墙的状态service firewalld status# 临时关闭防火墙service firewalld stop 修改 /abs/app/kafka_2.11-0.9.0.0/config 目录下的 server.properties 1234567891011121314# 指定broker绑定的主机名host.name=hadoop# broker访问producers和consumers的主机名，如果没设置则采用host.name# 需要注意的是这里默认是localhost，通过外部API来调用的话会报错# 所以在这里需要指定他为服务器端的IP地址advertised.host.name=192.168.10.100# 将日志存放目录更改为自己指定的目录# 因为默认的目录是在临时文件夹中，在服务器关机或重启之后会清除log.dirs=/abs/app/tmp/kafka-logs# 指定zookeeperzookeeper.connect=hadoop:2181 以上设置无误后在服务器端开启服务如下: 12345678# 启动ZooKeeper 在/abs/app/zookeeper-3.4.5-cdh5.7.0/bin 输入./zkServer.sh start# 启动Kafka 在/abs/app/kafka_2.11-0.9.0.0/bin 输入kafka-server-start.sh $KAFKA_HOME/config/server.properties# 启动消费者kafka-console-consumer.sh --zookeeper hadoop:2181 --topic hello_topic 运行 KafkaClientApp ，可以在服务器的消费者端口看到本地发送的消息 Consumer API 的使用新建 Java 类参照目录结构在 com.share.spark.kafka 包中新建 KafkaConsumer 类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.share.spark.kafka;import kafka.consumer.Consumer;import kafka.consumer.ConsumerConfig;import kafka.consumer.ConsumerIterator;import kafka.consumer.KafkaStream;import kafka.javaapi.consumer.ConsumerConnector;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;/** * Kafka消费者 */public class KafkaConsumer extends Thread&#123; private String topic; public KafkaConsumer(String topic)&#123; this.topic = topic; &#125; private ConsumerConnector createConnector()&#123; Properties properties = new Properties(); properties.put("zookeeper.connect",KafkaProperties.ZK); properties.put("group.id",KafkaProperties.GROUP_ID); return Consumer.createJavaConsumerConnector(new ConsumerConfig(properties)); &#125; @Override public void run() &#123; ConsumerConnector consumer = createConnector(); Map&lt;String,Integer&gt; topicCountMap = new HashMap&lt;String,Integer&gt;(); topicCountMap.put(topic,1); //String: topic //List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; 对应的数据流 Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; messageStream = consumer.createMessageStreams(topicCountMap); //get(0)为获取我们每次接收到的数据 KafkaStream&lt;byte[], byte[]&gt; stream = messageStream.get(topic).get(0); //迭代 ConsumerIterator&lt;byte[], byte[]&gt; iterator = stream.iterator(); while (iterator.hasNext())&#123; String message = new String(iterator.next().message()); System.out.println("rec: "+ message); &#125; &#125;&#125; 在 KafkaClientApp 类中添加启动&nbsp;Consumer&nbsp;的代码 12345678910111213package com.share.spark.kafka;/** * Kafka API 测试 */public class KafkaClientApp &#123; public static void main(String[] args) &#123; new KafkaProducer(KafkaProperties.TOPIC).start(); new KafkaConsumer(KafkaProperties.TOPIC).start(); &#125;&#125; 准备与测试说明: 服务器为 CentOS7,本地为 Windows10 为避免不必要的错误请先关闭 SELinux 本地访问服务器需要对服务器的防火墙进行相关设置(这里采用临时关闭防火墙) 12345# 查看防火墙的状态service firewalld status# 临时关闭防火墙service firewalld stop 修改 /abs/app/kafka_2.11-0.9.0.0/config 目录下的 server.properties 12345678910111213141516# 指定broker绑定的主机名host.name=hadoop# broker访问producers和consumers的主机名，如果没设置则采用host.name# 需要注意的是这里默认是localhost，通过外部API来调用的话会报错# 所以在这里需要指定他为服务器端的IP地址advertised.host.name=192.168.10.100# 将日志存放目录更改为自己指定的目录# 因为默认的目录是在临时文件夹中，在服务器关机或重启之后会清除log.dirs=/abs/app/tmp/kafka-logs# 指定zookeeper# 在测试过程中出现ZooKeeper连接问题，把主机名改成IP后解决# zookeeper.connect=hadoop:2181zookeeper.connect=192.168.10.100:2181 以上设置无误后在服务器端开启服务如下: 12345678# 启动ZooKeeper 在/abs/app/zookeeper-3.4.5-cdh5.7.0/bin 输入./zkServer.sh start# 启动Kafka 在/abs/app/kafka_2.11-0.9.0.0/bin 输入kafka-server-start.sh $KAFKA_HOME/config/server.properties# 启动消费者kafka-console-consumer.sh --zookeeper hadoop:2181 --topic hello_topic 运行 KafkaClientApp ，可以在 IDEA 控制台上看到发送消息和接收消息 也可以在服务器的消费者端口看到本地发送的消息]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 环境部署]]></title>
    <url>%2FKafka%2FKafka-%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前期准备Kafka概述访问 Kafka 官网了解 Kafka 是什么、能做什么、如何使用。 Kafka 安装前置条件安装 ZooKeeper 下载 Kafka 与解压下载地址 找到 kafka_2.11-0.9.0.0.tgz 并下载 将他传输到 /abs/software 目录 解压到指定目录 123#将Kafka解压到/abs/app/中tar -zxvf kafka_2.11-0.9.0.0.tgz -C /abs/app/ 配置 kafka 环境变量将 kafka 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport KAFKA_HOME=/abs/app/kafka_2.11-0.9.0.0export PATH=$KAFKA_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile Kafka单节点单broker的部署及使用修改配置文件在 /abs/app/tmp 目录下新建 kafka-logs 文件夹: 1mkdir kafka-logs 在 /abs/app/kafka_2.11-0.9.0.0/config 目录中对 server.properties 进行修改 123456789101112131415161718192021# 编辑配置文件vi server.properties# The id of the broker. This must be set to a unique integer for each broker.broker.id=0# The port the socket server listens on#port=9092listeners=PLAINTEXT://:9092# Hostname the broker will bind to. If not set, the server will bind to all interfaceshost.name=hadoop# A comma seperated list of directories under which to store log fileslog.dirs=/abs/app/tmp/kafka-logs# The default number of log partitions per topic. More partitions allow greaternum.partitions=1# root directory for all kafka znodes.zookeeper.connect=hadoop:2181 启动 Kafka先启动 ZooKeeper 参照 Kafka 官网快速启动 在前台启动单节点单 broker 的 Kafka 在 /abs/app/kafka_2.11-0.9.0.0/bin 目录中进行如下操作: 1kafka-server-start.sh $KAFKA_HOME/config/server.properties 验证启动 Kafka用 jps 查看当前所有的 Java 进程 pid如下: 123456789[root@hadoop tmp]# jps2448 QuorumPeerMain2595 Jps2538 Kafka[root@hadoop tmp]# jps -m2448 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2538 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server.properties2606 Jps -m 可以看到 Kafka 进程已经启动 创建一个 topic1kafka-topics.sh --create --zookeeper hadoop:2181 --replication-factor 1 --partitions 1 --topic hello_topic (PS: 创建 topic 相当于指定 zookeeper) 查看所有 topic1kafka-topics.sh --list --zookeeper hadoop:2181 发送消息1kafka-console-producer.sh --broker-list hadoop:9092 --topic hello_topic (PS: 发送消息相当于指定 broker) 消费消息1kafka-console-consumer.sh --zookeeper hadoop:2181 --topic hello_topic --from-beginning (PS: 消费消息相当于指定 zookeeper 。 from-beginning 的意思为是否从开始消费，根据实际需求决定是否添加) 验证发送消息与消费消息在发送端发送消息，看看在消费消息端是否能看到。 查看 topic 详情查看所有 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 查看指定 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic hello_topic Kafka单节点多broker的部署及使用修改配置文件参照 Kafka 官网快速启动进行相关配置 在 /abs/app/kafka_2.11-0.9.0.0/config 目录中为 broker 新建三个配置文件 1234# make a config file for each of the brokerscp server.properties server-1.propertiescp server.properties server-2.propertiescp server.properties server-3.properties 对 server-1.properties 、 server-2.properties 、 server-3.properties 进行相关配置 1234567891011121314151617181920# config server-1.propertiesvi server-1.propertiesbroker.id=1listeners=PLAINTEXT://:9093log.dirs=/abs/app/tmp/kafka-logs-1# config server-2.propertiesvi server-2.propertiesbroker.id=2listeners=PLAINTEXT://:9094log.dirs=/abs/app/tmp/kafka-logs-2# config server-3.propertiesvi server-3.propertiesbroker.id=3listeners=PLAINTEXT://:9095log.dir=/tmp/kafka-logs-3 启动 Kafka先启动 ZooKeeper 参照 Kafka 官网快速启动 在后台启动单节点多 broker 的 Kafka 在 /abs/app/kafka_2.11-0.9.0.0/bin 目录中进行如下操作: 12345kafka-server-start.sh -daemon $KAFKA_HOME/config/server-1.properties &amp;kafka-server-start.sh -daemon $KAFKA_HOME/config/server-2.properties &amp;kafka-server-start.sh -daemon $KAFKA_HOME/config/server-3.properties &amp; 验证启动 Kafka用 jps 查看当前所有的 Java 进程 pid如下: 1234567891011121314[root@hadoop config]# jps2577 Kafka2451 QuorumPeerMain2520 Kafka2633 Kafka2686 Jps[3]+ Done kafka-server-start.sh -daemon $KAFKA_HOME/config/server-3.properties[root@hadoop config]# jps -m2577 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-2.properties2451 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2520 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-1.properties2633 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-3.properties2697 Jps -m 可以看到 Kafka 进程已经启动。 创建一个 topic1kafka-topics.sh --create --zookeeper hadoop:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic (PS: 创建 topic 相当于指定 zookeeper，这里是一个分区三个副本) 查看所有 topic1kafka-topics.sh --list --zookeeper hadoop:2181 发送消息1kafka-console-producer.sh --broker-list hadoop:9093,hadoop:9094,hadoop:9095 --topic my-replicated-topic (PS: 发送消息相当于指定 broker) 消费消息1kafka-console-consumer.sh --zookeeper hadoop:2181 --topic my-replicated-topic (PS: 消费消息相当于指定 zookeeper 。 from-beginning 的意思为是否从开始消费，根据实际需求决定是否添加) 验证发送消息与消费消息在发送端发送消息，看看在消费消息端是否能看到。 查看 topic 详情查看所有 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 查看指定 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic my-replicated-topic Kafka 容错性测试前期准备容错测试基于 Kafka单节点多broker的部署及使用 查看指定 topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic my-replicated-topic 结果如下: 12Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 可以得出有一个副本、三个分区，主节点是1号分区，在运行着的有1、2、3 用 jps -m 查看当前所有的 Java 进程 pid 12345672640 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-3.properties2738 ConsoleProducer --broker-list hadoop:9093,hadoop:9094,hadoop:9095 --topic my-replicated-topic2581 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-2.properties2759 ConsoleConsumer --zookeeper hadoop:2181 --topic my-replicated-topic2522 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-1.properties2492 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2958 Jps -m 验证发送消息与消费消息1关闭从节点的进程 server-2.properties ，进程号可以看出是 2581，命令如下: 1kill -9 2581 再查看当前所有的 Java 进程 1234567[root@hadoop tmp]# jps -m2640 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-3.properties2738 ConsoleProducer --broker-list hadoop:9093,hadoop:9094,hadoop:9095 --topic my-replicated-topic2759 ConsoleConsumer --zookeeper hadoop:2181 --topic my-replicated-topic2522 Kafka /abs/app/kafka_2.11-0.9.0.0/config/server-1.properties2492 QuorumPeerMain /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg2990 Jps -m 在发送端发送消息，看看在消费消息端是否能看到。 经过测试发现可以从消费信息端接收到发送消息端发出的消息。 然而查看 my-replicated-topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic my-replicated-topic 结果如下: 12Topic:my-replicated-topic PartitionCount:1 Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 1 Replicas: 1,2,3 Isr: 1,3 可以得出有一个副本、三个分区，主节点是1号分区，在运行着的有1、3 2号从节点的故障并没有影响 Kafka 的正常运行 验证发送消息与消费消息2关闭主节点的进程 server-1.properties ，进程号可以看出是 2522，命令如下: 1kill -9 2522 在发送端发送消息，看看在消费消息端是否能看到。 经过测试发现可以从消费信息端接收到发送消息端发出的消息。 然而查看 my-replicated-topic 详情 1kafka-topics.sh --describe --zookeeper hadoop:2181 --topic my-replicated-topic 结果如下: 12Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 3 Replicas: 1,2,3 Isr: 3 可以得出有一个副本、三个分区，主节点是3号，在运行着的只有3号节点 1、2号从节点的故障并没有影响 Kafka 的正常运行，自动选举3号节点为主节点 Kafka 的容错性得到了证明]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 的安装]]></title>
    <url>%2FZooKeeper%2FZooKeeper-%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装步骤下载 ZooKeeper 与解压ZooKeeper 下载地址 找到 zookeeper-3.4.5-cdh5.7.0.tar.gz 将他传输到 /abs/software 目录 解压到指定目录 123#将ZooKeeper解压到/abs/app/中tar -zxvf zookeeper-3.4.5-cdh5.7.0.tar.gz -C /abs/app/ 配置 ZooKeeper 环境变量将 ZooKeeper 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport ZOOKEEPER_HOME=/abs/app/zookeeper-3.4.5-cdh5.7.0export PATH=$ZOOKEEPER_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 配置 ZooKeeper在 /abs/app/zookeeper-3.4.5-cdh5.7.0/conf 目录对 ZooKeeper 进行相关配置 123456cp zoo_sample.cfg zoo.cfgvi zoo.cfg# 修改数据存储文件目录(需要自己建相关目录)dataDir=/abs/app/tmp/zk 验证在 /abs/app/zookeeper-3.4.5-cdh5.7.0/bin 目录输入 ./zkServer.sh start 验证 在命令行显示如下: 123JMX enabled by defaultUsing config: /abs/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 在命令行输入 jps 122563 QuorumPeerMain2581 Jps QuorumPeerMain 就是 ZooKeeper 的进程 连接 ZooKeeper 12345678./zkCli.sh[zk: localhost:2181(CONNECTED) 6] ls /[zookeeper][zk: localhost:2181(CONNECTED) 7] ls /zookeeper[quota][zk: localhost:2181(CONNECTED) 8] ls /zookeeper/quota[] 以上操作没问题就说明安装配置成功。]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume 实战练习]]></title>
    <url>%2FFlume%2FFlume-%E5%AE%9E%E6%88%98%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前期准备了解Flume 架构及核心组件Flume 架构及核心组件 Source : 收集（指定数据源从哪里获取） Channel : 聚集 Sink : 输出（把数据写到哪里去） 学习使用 Flume通过一个简单的小例子学习使用 Flume 使用 Flume 的关键就是写配置文件 配置文件的构成： A) 配置 SourceB) 配置 ChannelC) 配置 SinkD) 把以上三个组件串起来 A simple example 123456789101112131415161718192021222324252627282930313233# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# a1: agent 的名称# r1: source 的名称# k1: sink 的名称# c1: channel 的名称# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# type: source组件的类型# bind: source绑定的主机或IP# port: source绑定的端口号# Describe the sinka1.sinks.k1.type = logger# 把日志输出到控制台# Use a channel which buffers events in memorya1.channels.c1.type = memory# 存放在内存队列# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1# r1的channels指定到c1# k1的channel从c1得到# 一个source可以输出到多个channel# 一个channel只能输出一个sink 实战一需求需求：从指定网络端口采集数据输出到控制台 写配置文件在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录中新建 example.conf 如下: 1234567891011121314151617181920# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = hadoopa1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 agentFlume 官网启动 agent 的命令: 1$ bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template agent options: 123--name,-n &lt;name&gt; the name of this agent (required)--conf,-c &lt;conf&gt; use configs in &lt;conf&gt; directory--conf-file,-f &lt;file&gt; specify a config file (required if -z missing) 实际用的启动 agent 的命令: 1flume-ng agent -n a1 -c $FLUME_HOME $FLUME_HOME/conf/example.conf -Dflume.root.logger=INFO,console // Dflume.root.logger=INFO,console 为将输出结果显示到控制台 启动失败12345Info: Including Hive libraries found via () for Hive access+ exec /abs/app/jdk1.8.0_161/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp '/abs/app/apache-flume-1.6.0-cdh5.7.0-bin:/abs/app/apache-flume-1.6.0-cdh5.7.0-bin/lib/*:/lib/*' -Djava.library.path= org.apache.flume.node.Application -n a1 -f /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf/example.conflog4j:WARN No appenders could be found for logger (org.apache.flume.lifecycle.LifecycleSupervisor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 上网查了一下，别人是 -c 的路径指定错误，我的也错了。 -c 后面跟的是 Flume 的 conf 目录 所以正确的启动命令为: 1flume-ng agent -n a1 -c $FLUME_HOME/conf -f $FLUME_HOME/conf/example.conf -Dflume.root.logger=INFO,console 正常启动后可以看到如下: 可以看到 Sink 和 Source 都启动了 绑定的主机名为 hadoop 的 IP 和绑定的端口号都有显示 验证12[root@hadoop ~]# telnet hadoop 44444-bash: telnet: command not found 显示找不到 telnet ，用 yum install telnet 安装telnet telnet 进入 hadoop 的 44444 端口进行输入单词按 Enter agent 的那一端显示如下: 从图中可以看到如下: 1Event: &#123; headers:&#123;&#125; body: 73 70 61 72 6B 0D spark. &#125; Event 是 Flume 数据传输的基本单元Event = 可选的 header + byte array 以上实现了从指定网络端口采集数据输出到控制台的需求。 实战二需求需求：监控一个文件实时采集新增的数据输出到控制台 根据需求可以采用以下方案实现： Agent 选型: exec source + memory channel + logger sink 写配置文件在 /abs/data 目录新建 data.log 1touch data.log 在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录中新建 exec-memory-logger.conf 如下: 1234567891011121314151617181920# exec-memory-logger.conf: A realtime single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /abs/data/data.loga1.sources.r1.shell = /bin/sh -c# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 agent Flume 启动 agent 的命令: 1flume-ng agent -n a1 -c $FLUME_HOME/conf -f $FLUME_HOME/conf/exec-memory-logger.conf -Dflume.root.logger=INFO,console // Dflume.root.logger=INFO,console 为将输出结果显示到控制台 正常启动后可以看到如下: 可以看到 Source 、 Channel 和 Sink 的类型和启动类型以及 Source 要执行的命令 验证在 /abs/data 目录输入 echo hello &gt;&gt; data.log agent 的那一端显示如下: 以上实现了监控一个文件实时采集新增的数据输出到控制台的需求。 拓展参照 Flume 用户指南 如果用 Flume 采集数据做离线处理，可以使用 HDFS Sink 如果用 Flume 采集数据做实时处理，可以使用 Kafka Sink 这里只提供一个拓展，根据具体的需求使用。 实战三需求需求：将 A 服务器上的日志实时采集到 B 服务器 根据需求可以采用以下方案实现： Agent A 选型: exec source + memory channel + avro sinkAgent B 选型: avro source + memory channel + logger sink 写配置文件在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录中新建如下配置文件: exec-memory-avro.conf: 12345678910111213141516171819202122# exec-memory-avro.conf: A realtime Flume configuration# Name the components on this agentexec-memory-avro.sources = exec-sourceexec-memory-avro.sinks = avro-sinkexec-memory-avro.channels = memory-channel# Describe/configure the sourceexec-memory-avro.sources.exec-source.type = execexec-memory-avro.sources.exec-source.command = tail -F /abs/data/data.logexec-memory-avro.sources.exec-source.shell = /bin/sh -c# Describe the sinkexec-memory-avro.sinks.avro-sink.type = avroexec-memory-avro.sinks.avro-sink.hostname = hadoopexec-memory-avro.sinks.avro-sink.port = 44444# Use a channel which buffers events in memoryexec-memory-avro.channels.memory-channel.type = memory# Bind the source and sink to the channelexec-memory-avro.sources.exec-source.channels = memory-channelexec-memory-avro.sinks.avro-sink.channel = memory-channel avro-memory-logger.conf: 1234567891011121314151617181920# avro-memory-logger.conf: A realtime Flume configuration# Name the components on this agentavro-memory-logger.sources = avro-sourceavro-memory-logger.sinks = logger-sinkavro-memory-logger.channels = memory-channel# Describe/configure the sourceavro-memory-logger.sources.avro-source.type = avroavro-memory-logger.sources.avro-source.bind = hadoopavro-memory-logger.sources.avro-source.port = 44444# Describe the sinkavro-memory-logger.sinks.logger-sink.type = logger# Use a channel which buffers events in memoryavro-memory-logger.channels.memory-channel.type = memory# Bind the source and sink to the channelavro-memory-logger.sources.avro-source.channels = memory-channelavro-memory-logger.sinks.logger-sink.channel = memory-channel 启动 agent两个 Agent ,先启动 Agent A ,再启动 Agent B 先启动 avro-memory-logger: 1flume-ng agent -n avro-memory-logger -c $FLUME_HOME/conf -f $FLUME_HOME/conf/avro-memory-logger.conf -Dflume.root.logger=INFO,console 再启动 exec-memory-avro: 1flume-ng agent -n exec-memory-avro -c $FLUME_HOME/conf -f $FLUME_HOME/conf/exec-memory-avro.conf -Dflume.root.logger=INFO,console 验证在 /abs/data/ 目录中输入以下命令： 12echo hello spark &gt;&gt; data.logecho Valentine &gt;&gt; data.log Agent avro-memory-logger 显示如下： 以上实现了将 A 服务器上的日志实时采集到 B 服务器的需求。 这里采用的是一个服务器开三个窗口，有条件的可以尝试用两台服务器进行这个实战练习]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
        <tag>Practise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume 环境部署]]></title>
    <url>%2FFlume%2FFlume-%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[步骤Flume概述访问 Flume 官网了解 Flume 是什么、能做什么、如何使用。 Flume 安装前置条件Flume 安装前置条件如下： 123456789101112131415Java Runtime Environment - Java 1.8 or later#Java 运行环境 - Java 1.8 或以上 Memory - Sufficient memory for configurations used by sources, channels or sinks#内存 - 足够的内存配置供 sources、channels、sinks 使用Disk Space - Sufficient disk space for configurations used by channels or sinks#磁盘空间 - 足够的磁盘空间配置供 sources、channels、sinks 使用Directory Permissions - Read/Write permissions for directories used by agent# 目录权限 - 赋予 agent 对目录的读写权限 安装 JDK参照 ： Java8 的安装 下载 Flume 与解压下载地址 找到 flume-ng-1.6.0-cdh5.7.0.tar.gz 将他传输到 /abs/software 目录 解压到指定目录 123#将Flume解压到/abs/app/中tar -zxvf flume-ng-1.6.0-cdh5.7.0.tar.gz -C /abs/app/ 配置 Flume 环境变量将 Flume 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport FLUME_HOME=/abs/app/apache-flume-1.6.0-cdh5.7.0-binexport PATH=$FLUME_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 配置 Flume在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/conf 目录对 Flume 进行相关配置 拷贝 Flume 环境配置脚本 1cp flume-env.sh.template flume-env.sh 1234567#编辑 flume-env.shvi flume-env.sh# 在# export JAVA_HOME=/usr/lib/jvm/java-6-sun下面添加如下配置export JAVA_HOME=/abs/app/jdk1.8.0_161 验证在 /abs/app/apache-flume-1.6.0-cdh5.7.0-bin/bin 目录输入 flume-ng version 验证 出现以下内容说明安装配置成功。 12345Flume 1.6.0-cdh5.7.0Source code repository: https://git-wip-us.apache.org/repos/asf/flume.gitRevision: 8f5f5143ae30802fe79f9ab96f893e6c54a105d1Compiled by jenkins on Wed Mar 23 11:38:48 PDT 2016From source with checksum 50b533f0ffc32db9246405ac4431872e]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 CentOS7.2 环境编译 Spark-2.2.0 源码]]></title>
    <url>%2FSpark%2F%E5%9F%BA%E4%BA%8E-CentOS7-2-%E7%8E%AF%E5%A2%83%E7%BC%96%E8%AF%91-Spark-2-2-0-%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[编译 Spark-2.2.0 源码参考 基于CentOS6.4环境编译Spark-2.1.0源码 参考 Spark 2.2.0下载安装及源码编译 需求分析实际工作中，Spark 官网所提供的安装包不能满足我们的需求。因为环境的不同出现问题所以必须要结合实际环境用 Spark 源码进行编译后使用。 根据 Spark 官方文档编译模块的介绍如下： 1The Maven-based build is the build of reference for Apache Spark. Building Spark using Maven requires Maven 3.3.9 or newer and Java 8+. Note that support for Java 7 was removed as of Spark 2.2.0. 得出编译 Spark-2.2.0 需要 Maven 3.3.9 和 Java 8+ 前期准备 Java8 的安装 Maven3.3.9 的安装 下载maven后进行环境变量的设置，设置maven的内存使用,在环境变量中加入如下命令 export MAVEN_OPTS=&quot;-Xmx2g -XX:ReservedCodeCacheSize=512m&quot; Scala 的安装 git：直接输入命令：sudo yum install git 下载 git Spark-2.2.0 源码下载Spark 下载地址 解压将下载好的 spark-2.2.0.tgz 通过 Xftp 传输到 /abs/software 目录 解压 spark-2.2.0.tgz 123# 将Spark源码包解压到/abs/app/中tar -zxvf spark-2.2.0.tgz -C /abs/app/ 解压后的目录机构如下所示： 配置添加 CDH 的 maven repository 到 pom.xml 12345&lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;name&gt;cloudera Repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt; 修改位于 /abs/app/spark-2.2.0/dev 的 make-distribution.sh 文件 12345678910111213141516171819202122# 注释以下内容:#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v "INFO" | tail -n 1)#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | tail -n 1)#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | tail -n 1)#SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v "INFO"\# | fgrep --count "&lt;id&gt;hive&lt;/id&gt;";\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use "set -o pipefail"# echo -n)# 加入下面的内容:VERSION=2.2.0SCALA_VERSION=2.11SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0SPARK_HIVE=1 编译查看 Spark 官方文档编译源码部分 我们可以使用 Spark 源码目录中的 dev 下的 make-distribution.sh 脚本，官方提供的编译命令如下： 1./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phadoop-2.7 -Phive -Phive-thriftserver -Pmesos -Pyarn 可以根据具体的条件来编译 Spark，比如我们使用的 Hadoop 版本是 2.6.0-cdh5.7.0，并且我们需要将 Spark 运行在 YARN 上、支持对 Hive 的操作，那么我们的 Spark 源码编译脚本就是： 1./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn 编译成功后的界面效果 123456789101112131415161718192021222324252627282930313233343536373839404142main:[INFO] Executed tasks[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] Spark Project Parent POM ........................... SUCCESS [ 26.000 s][INFO] Spark Project Tags ................................. SUCCESS [ 18.265 s][INFO] Spark Project Sketch ............................... SUCCESS [ 20.524 s][INFO] Spark Project Networking ........................... SUCCESS [ 35.170 s][INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 20.208 s][INFO] Spark Project Unsafe ............................... SUCCESS [ 29.250 s][INFO] Spark Project Launcher ............................. SUCCESS [ 32.780 s][INFO] Spark Project Core ................................. SUCCESS [07:07 min][INFO] Spark Project ML Local Library ..................... SUCCESS [ 50.509 s][INFO] Spark Project GraphX ............................... SUCCESS [ 42.277 s][INFO] Spark Project Streaming ............................ SUCCESS [03:55 min][INFO] Spark Project Catalyst ............................. SUCCESS [03:49 min][INFO] Spark Project SQL .................................. SUCCESS [05:04 min][INFO] Spark Project ML Library ........................... SUCCESS [03:16 min][INFO] Spark Project Tools ................................ SUCCESS [ 10.737 s][INFO] Spark Project Hive ................................. SUCCESS [02:04 min][INFO] Spark Project REPL ................................. SUCCESS [ 12.745 s][INFO] Spark Project YARN Shuffle Service ................. SUCCESS [ 27.419 s][INFO] Spark Project YARN ................................. SUCCESS [ 29.455 s][INFO] Spark Project Hive Thrift Server ................... SUCCESS [01:28 min][INFO] Spark Project Assembly ............................. SUCCESS [ 15.889 s][INFO] Spark Project External Flume Sink .................. SUCCESS [ 24.513 s][INFO] Spark Project External Flume ....................... SUCCESS [ 26.800 s][INFO] Spark Project External Flume Assembly .............. SUCCESS [ 5.247 s][INFO] Spark Integration for Kafka 0.8 .................... SUCCESS [ 28.671 s][INFO] Kafka 0.10 Source for Structured Streaming ......... SUCCESS [ 26.704 s][INFO] Spark Project Examples ............................. SUCCESS [01:44 min][INFO] Spark Project External Kafka Assembly .............. SUCCESS [ 6.346 s][INFO] Spark Integration for Kafka 0.10 ................... SUCCESS [ 31.861 s][INFO] Spark Integration for Kafka 0.10 Assembly .......... SUCCESS [ 5.706 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 37:22 min[INFO] Finished at: 2018-03-08T22:01:29+08:00[INFO] Final Memory: 91M/440M[INFO] ------------------------------------------------------------------------ 可以在 /abs/app/spark-2.2.0 目录中看到生成了 spark-2.2.0-bin-2.6.0-cdh5.7.0.tgz 将 spark-2.2.0-bin-2.6.0-cdh5.7.0.tgz 移动到 /abs/software/ 1mv spark-2.2.0-bin-2.6.0-cdh5.7.0.tgz /abs/software/ 小结编译 Spark 源码没有想象中的那么简单，感谢解决问题的前辈们把自己的解决方案发布在互联网。 也希望有人能从我这里得到解决问题的思路。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Compile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven3.3.9 的安装]]></title>
    <url>%2FJava%2FMaven3-3-9-%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Maven3.3.9 在 Windows10 中的安装与配置说明为保证开发过程中版本的一致性，在虚拟机中 Maven 的版本为 3.3.9 所以物理机中 Maven 的版本为 3.3.9 版本不一致的时候先卸载 Maven ，再安装合适的版本 JDK 的版本也要一致，避免不必要的问题出现 卸载 Maven由于在物理机中的 Maven 版本不一致，所以先卸载再安装。 Maven 由于安装的时候只是解压，配置环境变量，设置本地仓库，所以卸载的时候也很简单 卸载过程如下： 1、删除解压的 Maven 文件夹； 2、右键我的电脑-属性-高级系统设置-环境变量-系统变量-Path，删除path里添加的 Maven 环境变量 D:\ST\IDEA\apache-maven-3.5.0\bin 3、删除本地仓库； 下载与解压 Maven 安装包Maven3.3.9 安装包下载地址 下载 apache-maven-3.3.9-bin.zip 在相应目录中解压 apache-maven-3.3.9-bin.zip 配置 Maven 环境变量右键我的电脑-属性-高级系统设置-环境变量-系统变量-Path 添加一条环境变量的记录 D:\ST\IDEA\apache-maven-3.3.9\bin 验证 Maven 安装的有效性按 Windows + r 输入 cmd 进入命令行界面 在命令行中输入 mvn -v 验证，出现以下内容说明安装配置成功。 123456Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: D:\ST\IDEA\apache-maven-3.3.9\bin\..Java version: 1.8.0_161, vendor: Oracle CorporationJava home: D:\ST\JAVA\jdk\route\jreDefault locale: zh_CN, platform encoding: GBKOS name: "windows 10", version: "10.0", arch: "amd64", family: "dos" Maven3.3.9 在 CentOS7 中的安装与配置下载与解压 Maven 安装包Maven3.3.9 安装包下载地址 下载 apache-maven-3.3.9-bin.tar.gz 用 Xftp 将本地下载好的 JDK 包传输到 /abs/software 目录 解压到指定目录 123# 将Maven解压到/abs/app/中tar -zxvf apache-maven-3.3.9-bin.tar.gz -C /abs/app/ 配置 Maven 环境变量将 Maven 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport MAVEN_HOME=/abs/app/apache-maven-3.3.9export PATH=$MAVEN_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 验证 Maven 安装的有效性在命令行中输入 mvn -v 验证，出现以下内容说明安装配置成功。 123456Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: /abs/app/apache-maven-3.3.9Java version: 1.8.0_161, vendor: Oracle CorporationJava home: /abs/app/jdk1.8.0_161/jreDefault locale: en_US, platform encoding: UTF-8OS name: "linux", version: "3.10.0-327.el7.x86_64", arch: "amd64", family: "unix" 修改 Maven 本地仓库在 /abs 中新建 maven_repos 1mkdir maven_repos 在 /abs/app/apache-maven-3.3.9/conf 目录中修改 settings.xml 1234567# 在 &lt;!-- localRepository# | The path to the local repository maven will use to store artifacts.# |# | Default: $&#123;user.home&#125;/.m2/repository# &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;# --&gt;下添加内容如下:&lt;localRepository&gt;/abs/maven_repos&lt;/localRepository&gt; 以上是为 Maven 指定本地仓库的位置]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 的安装]]></title>
    <url>%2FJava%2FJava8-%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Java8 在 Windows10 中的安装与配置下载与安装 JDK在 Oracle 官网下载 JDK 在 Windows 下的安装包 jdk-8u161-windows-x64.exe 将 jdk-8u161-windows-x64.exe 安装到 D:\ST\JAVA\jdk\route 配置 Java 环境变量右键我的电脑-属性-高级系统设置-环境变量-系统变量 进行如下配置: 12345678# 新建 JAVA_HOMED:\ST\JAVA\jdk\route# 新建 CLASS_PATHD:\ST\JAVA\jdk\route\lib# 添加一条 Path 的值D:\ST\JAVA\jdk\route\bin 验证 Java 安装的有效性在命令行中输入 java -version 验证，出现以下内容说明安装配置成功。 123java version "1.8.0_161"Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode) Java8 在 CentOS7 中的安装与配置下载与解压 JDK在 Oracle 官网下载 JDK 在 Linux 下的安装包 jdk-8u161-linux-x64.tar.gz 在根目录下创建 /abs/software 目录，用 Xftp 将本地下载好的 JDK 包传输到 /abs/software 目录 解压到指定目录 123# 将JDK解压到/abs/app/中tar -zxvf jdk-8u161-linux-x64.tar.gz -C /abs/app/ 配置 Java 环境变量将 JDK 目录添加到系统环境变量(~/.bash_profile)中 1234vi ~/.bash_profileexport JAVA_HOME=/abs/app/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATH 让配置生效 1source ~/.bash_profile 验证 Java 安装的有效性在命令行中输入 java -version 验证，出现以下内容说明安装配置成功。 123java version "1.8.0_161"Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LNMP 网站无法访问的解决方法]]></title>
    <url>%2F%E8%8A%92%E6%9E%9C%E5%B8%83%E4%B8%81%2FLNMP-%E7%BD%91%E7%AB%99%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[解决方案问题说明之前搭建基于 LNMP + WordPress 的博客，到目前为止出现过两次 Error establishing a database connection 无法访问网站 解决过程// 2017.10.30 在排除了服务器的问题之后 初步推断是 MySQL 数据库连接的问题 查看 MySQL 日志 在 /var/log/mysqld.log 中发现以下 1234567892017-10-30 11:49:45 9260 [ERROR] InnoDB: Cannot allocate memory for the buffer pool2017-10-30 11:49:45 9260 [ERROR] Plugin 'InnoDB' init function returned error.2017-10-30 11:49:45 9260 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.2017-10-30 11:49:45 9260 [ERROR] Unknown/unsupported storage engine: InnoDB2017-10-30 11:49:45 9260 [ERROR] Aborting 根据日志搜索 MySQL 出现 InnoDB: Cannot allocate memory for the buffer pool 参考 解决MySQL : InnoDB: Cannot allocate memory for the buffer pool 有人也遇到了相同的问题 原因是给的内存不够 期间查看和重启 MySQL 都出错，各种方法都解决不了 在 /etc/my.conf 中做以下修改，并重启云主机 1innodb_buffer_pool_size =256M 问题得到了解决。 // 2018.2.10 又出现相同的问题，由于把解决方案记录在博客上了，也没搜到合适的解决方案。 重启了一次云主机和 MySQL 问题得到了解决。 收获网站能正常访问了 删除不必要的插件 做好备份 做好备份 做好备份]]></content>
      <categories>
        <category>芒果布丁</category>
      </categories>
      <tags>
        <tag>Solution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 伪分布式安装]]></title>
    <url>%2FHadoop%2FHadoop-%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[说明安装环境12345操作系统: CentOS7 mini JDK 安装包: jdk-8u161-linux-x64.tar.gzHadoop 安装包: hadoop-2.6.0-cdh5.7.0.tar.gz 注意事项CentOS7 mini 需要配置双网卡 虚拟机里的 JDK 版本要和物理机的一致，从 Oracle 官网下载。 Hadoop 安装包下载地址，注意版本一致。 前期准备安装虚拟机安装虚拟机 CentOS7 mini，过程略。 下载相关安装包在注意事项中提供了相关资源下载的链接或方法。 验证环境给 CentOS7 mini 配置双网卡 在虚拟机中 ping 物理机地址和 baidu.com ，验证其内外网的连通性。 通过 XShell 5 进行远程连接,方便后面的操作。 安装 JDK上传并解压安装包在 Oracle 官网下载 JDK 在 Linux 下的安装包 jdk-8u161-linux-x64.tar.gz 在根目录下创建 app 目录，用 Xftp 将本地下载好的 JDK 包传输到 app 目录 解压到指定目录 1tar -zxvf jdk-8u161-linux-x64.tar.gz -C /app/ 配置 Java 环境变量配置系统环境变量 1234vi ~/.bash_profileexport JAVA_HOME=/app/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATH 让配置生效1source ~/.bash_profile 在命令行中输入 java 验证。 安装 ssh安装命令1yum install ssh 生成 ssh 的 key12345678910111213# 生成 ssh 的 keyssh-keygen -t rsa用 ls -la 命令找到隐藏文件 .ssh 并 cd 进去# 备份 id_rsa.pubcp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys# 在 .ssh 目录下查看 authorized_keyscat authorized_keys 验证1234567# 免密码登陆ssh localhost# 退出exit Hadoop 环境配置下载、上传、解压 Hadoop 安装包Hadoop 安装包 hadoop-2.6.0-cdh5.7.0.tar.gz 下载地址 用 Xftp 把下载好的安装包上传到 /app 目录下 解压到指定目录 1tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C /app/ 修改配置文件在 /app/hadoop-2.6.0-cdh5.7.0/etc/hadoop 下修改配置文件 hadoop-env.sh 123#注释掉 export JAVA_HOME=$&#123;JAVA_HOME&#125; 并添加如下:export JAVA_HOME=/app/jdk1.8.0_161 core-site.xml 123456789&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://bs1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/app/tmp&lt;/value&gt;&lt;/property&gt; hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; slaves 123vi slaves# 把 localhost 改为你设定的主机名 启动 hdfs格式化 hdfs (PS: 仅第一次执行) 在 bin 目录下执行 1./hdfs namenode -format 在 sbin 目录下执行以下脚本启动 hdfs 1./start-dfs.sh 验证是否启动 hdfs 方法一: 1234567jps# 出现以下三个进程则为正常启动NameNodeDataNodeSecondaryNameNode 方法二: CentOS7 关闭防火墙，出于安全和方便的考虑，这里采用暂时关闭防火墙，实际应用时应设定相应的防火墙规则。 外部物理机应该在 hosts 文件中加入一条 “虚拟机主机名&nbsp;&nbsp;&nbsp;ip 地址”的解析。 12345678910111213141516171819#查看防火墙的状态systemctl status firewalld.service# 开启防火墙systemctl start firewalld.service# 关闭防火墙systemctl stop firewalld.service#开机启用防火墙systemctl enable firewalld.service#开机禁用防火墙systemctl disable firewalld.service 通过在物理机的浏览器地址栏输入以下访问 WebUI1http://虚拟机的主机名:50070 停止 hdfs在 sbin 目录下执行以下脚本停止 hdfs1./stop-dfs.sh 配置 Hadoop 环境变量配置系统环境变量 1234vi ~/.bash_profileexport HADOOP_HOME=/app/hadoop-2.6.0-cdh5.7.0export PATH=$HADOOP_HOME/bin:$PATH 让配置生效1source ~/.bash_profile 在命令行中输入 hadoop 验证。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>BD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 在 VirtualBox 中的克隆]]></title>
    <url>%2FLinux%2FCentOS7-%E5%9C%A8-VirtualBox-%E4%B8%AD%E7%9A%84%E5%85%8B%E9%9A%86%2F</url>
    <content type="text"><![CDATA[前言克隆 CentOS6 和克隆 CentOS7 过程大致相同，有些细节不一样。 思路在克隆CentOS虚拟机后发现网卡不显示，创建的是完整克隆，主机名和物理地址都一样，这里的想法是克隆出除 ip 地址、物理地址、主机名不一样其他都一样的虚拟机。 克隆根据需求创建链接克隆或完整克隆，这里采用的是完整克隆。 重新分配物理地址删除 /etc/sysconfig/network-scripts/ifcfg-enp0s3 文件中的物理地址 删除 /etc/sysconfig/network-scripts/ifcfg-enp0s8 文件中的物理地址 1删除两行：UUID 和物理地址 删除文件 /etc/udev/rules.d/70-persistent-ipoib.rules 1rm -rf /etc/udev/rules.d/70-persistent-ipoib.rules 修改 ifcfg-enp0s8 文件中的 ip这块网卡设置的是静态 ip，用来远程连接，过程略。 修改主机名CentOS7 修改主机名不同于 CentOS6 用 hostnamectl 命令查看或修改与主机名相关的配置，以下两个主机名设置一致。 123hostnamectl --static set-hostname [主机名]hostnamectl set-hostname [主机名] 手动更新/etc/hosts在文件最后增加一行:1IP 地址 主机名 重启linux重启启动 linux: 1init 6 测试进行内外网是否连通的测试 123ping -c 4 baidu.comping -c 4 192.168.10.x 以上完成了对虚拟机的有效克隆。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Clone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS6 在 VirtualBox 中的网络配置及克隆]]></title>
    <url>%2FLinux%2FCentOS6%E5%9C%A8VirtualBox%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%85%8B%E9%9A%86%2F</url>
    <content type="text"><![CDATA[前言与 VMware 相比， VirtualBox 安装命令行版的 Linux 操作系统占系统内存会相对较小。 网络配置NAT 模式下自动联网对于 CentOS 来说，配置网卡是一个比较重要的内容。 首先在新建虚拟机的时候我们给网卡配置成 NAT 模式； 登陆之后在命令行下输入 ifconfig 查看网卡的信息； 然后 ping 网关、baidu 等等，确保网络的畅通； 以上是采用 DHCP 获得的 ip 配置静态 IP将 DHCP 自动获取 ip 设置成静态 ip 1vi /etc/sysconfig/network-scripts/ifcfg-eth0 (PS:&nbsp;&nbsp;&nbsp;不同版本的CentOS的网卡名可能存在差异，以实际为主) 设置 ONBOOT=yes 将 dhcp 改为 static 依次添加 ip 地址、子网掩码、网关 123IPADDR=NETMASK=GATEWAY= (PS:&nbsp;&nbsp;&nbsp;以上 IP 设置自己根据外部网络适配器自己指定) 重启网卡1service network restart 测试网络是否畅通 配置 DNS 服务123vi /etc/resolv.confnameserver 8.8.8.8 (PS:&nbsp;&nbsp;&nbsp;nameserver 的地址根据实际情况改动) 以上完成了 CentOS6 网卡的配置 CentOS虚拟机克隆思路 在克隆 CentOS 虚拟机后发现网卡不显示，创建的是完整克隆，主机名和物理地址都一样，这里的想法是克隆出除 ip 地址、物理地址、主机名不一样其他都一样的虚拟机。 克隆根据需求创建链接克隆或完整克隆，这里采用的是完整克隆。 重新分配物理地址删除 /etc/sysconfig/network-scripts/ifcfg-eth0 文件中的物理地址 1删除两行： UUID 和物理地址 删除文件 /etc/udev/rules.d/70-persistent-net.rules 1rm -rf /etc/udev/rules.d/70-persistent-net.rules 修改主机名1vi /etc/sysconfig/network 重启 Linux1init 6 修改 hosts12vi /etc/hosts 文件。在文件最后增加一行 ：IP 地址 主机名 以上完成了对虚拟机的有效克隆]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Clone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 VirtualBox 中为 CentOS7mini 配置双网卡]]></title>
    <url>%2FLinux%2F%E5%9C%A8VirtualBox%E4%B8%AD%E4%B8%BACentOS7mini%E9%85%8D%E7%BD%AE%E5%8F%8C%E7%BD%91%E5%8D%A1%2F</url>
    <content type="text"><![CDATA[配置过程需求分析要同时满足虚拟机访问互联网和远程连接，需要配置两块网卡。 一块为 NAT 网络，这块用来访问互联网。 另一块为 Host-Only 网络，进行远程连接。 注意事项在 Virtualbox 中配置双网卡一定要先只开一个，等配好一个再配下一个，不然第二个不识别。 在全局设定里配置 NAT 网络、 Host-Only 网络 在新建虚拟机时选择网卡(NAT) 配置第一块网卡123456vi /etc/sysconfig/network-scripts/ifcfg-enp0s3改动 ONBOOT=yes重启网卡 service network restart测试能否访问外网 ping –c 4 baidu.com 给 CentOS7mini 装一些常用的指令CentOS7mini 中缺少 vim， wget， curl， lsof， ifconfig之类的指令，需要自己单独安装。执行： 1yum -y install nano vim wget curl net-tools lsof 如果提示出错执行 yum makecache 重建缓存即可 配置第二块网卡关机后添加 Host-Only 网卡，开机复制网卡文件后进行相关配置。 进入网卡文件夹的目录 /etc/sysconfig/network-scripts 后输入 cp ifcfg-enp0s3 ifcfg- enp0s8 ，通过 ls 查看是否添加上了对 ifcfg- enp0s8 作出如下修改： 123456BOOTPROTO=staticNAME=enp0s8DEVICE=enp0s8ONBOOT=yesIPADDR=192.168.10.10NETMASK=255.255.255.0 配置好之后重启网卡，ping 192.168.10.10 验证 Host-Only 网卡的有效性。 至此，在 CentOS7mini 中配置双网卡就完成了，可以根据这个配置作为别的操作的基础。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Different between equals and ==]]></title>
    <url>%2FJava%2FDifferent-between-equals-and%2F</url>
    <content type="text"><![CDATA[equals :&nbsp;&nbsp;&nbsp;判断逻辑上值是否相等== :&nbsp;&nbsp;&nbsp;判断内存引用是否相等 Demo]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FHexo%2FHello-World%2F</url>
    <content type="text"><![CDATA[这是一个基于 Hexo + Github Pages + NexT 搭建的静态博客。 在整个过程中遇到过许多问题，从最开始的搭建到后期的主题优化，用 “ 折腾 ” 二字来总结恰好合适。 现在网站外观基本上满足了我的需求，算是一个一般的皮囊吧，我更想让他有一个有趣的灵魂。 事件经过起源源于我对未知事物的好奇心，很久之前的某天在知乎上看到了一篇文章，萌生了搭建个人博客的想法，于是我开始按照他的教程开始了搭建，由于当时水平有限，一开始的尝试并未成功，尝试几次后也就搁置了，当我成功按这个方法搭建好的时候我已经在另一个平台成功搭建了自己的个人博客。 后来看了另一篇文章腾讯云的 1001 种玩法在云主机中搭建博客，注册了域名，租了云主机，基于 LNMP+WordPress 搭建了自己的个人博客，在上面做了一些笔记和更新了一些教程，在这个过程中学到了不少东西，关键是对整个过程有了一些概念为现在本站的搭建有了一些积极的作用。 因为原来的云主机有别的用途，所以根据网上的教程优化了主题，准备将原来的博客我认为有价值的博文迁移过来，在本站继续写。 最初的想法最初的想法很简单，在个人博客记录学习笔记，然后分享给有需要的人，最好是能产生一些沟通交流，还有就是总结我遇到的问题和解决方法。可能是博客的推广工作和搜索优化做的不到位的原因，最初的博客没有太大的访问量，评论更是寥寥无几。现在的想法是完善之前的不足，写出有价值的文章。 搭建及优化的过程搭建基于 Hexo + Github Pages + NexT 的静态博客主要参照的是 教你免费搭建个人博客，Hexo&amp;Github NexT主题优化主要参照的是 打造个性超赞博客Hexo+NexT+GithubPages的超深度优化 (包含怎么用Markdown写文章) / Hexo 折腾记 / Hexo+Next主题优化 遇到问题的解决方法遇到问题要善于运用搜索引擎，想清楚用什么关键词描述你的问题。 提问之前先看看 提问的智慧]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hello world</tag>
      </tags>
  </entry>
</search>
